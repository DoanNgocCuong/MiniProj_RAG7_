{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sếp @q... ơi, về RAG hiện có frameworks nào ngon hỗ trợ nhanh chuyển qua lại giữa các kỹ thuật không ạ. \n",
    "Ví dụ là: chuyển qua lại nhanh giữa các kỹ thuật Chunking, Retrieval, Generation, …\n",
    "q... — 5:01 PM\n",
    "chuyển qua lại nhanh là sao nhỉ\n",
    "@@\n",
    "Đoàn Ngọc Cường - AI — 5:06 PM\n",
    "Kiểu em muốn test nhanh với 1 PDF thì kĩ thuật Chunking khác nhau: Split, Semantic chunk, … thì kết quả khác nhau như nào. \n",
    "Hay Retrieval khi có mỗi BM25, Khi Hybrid Search, khi kết hợp thêm Rerank, … \n",
    "Đoàn Ngọc Cường - AI — 6:41 PM\n",
    ":partyhatsmiwe: :pepepray~1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frameworks RAG Hỗ Trợ Chuyển Đổi Nhanh Giữa Các Kỹ Thuật\n",
    "\n",
    "Để thử nghiệm và chuyển đổi nhanh giữa các kỹ thuật RAG (Retrieval Augmented Generation) khác nhau như Chunking, Retrieval, và Generation, có một số frameworks hiện đại hỗ trợ tốt. Dưới đây là phân tích về các frameworks phổ biến:\n",
    "\n",
    "## 1. LangChain\n",
    "\n",
    "LangChain cung cấp một hệ sinh thái linh hoạt để xây dựng ứng dụng RAG với khả năng thử nghiệm và chuyển đổi dễ dàng giữa các kỹ thuật khác nhau.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Document Transformers**: Hỗ trợ nhiều phương pháp chunking (character, token, semantic chunking, recursive)\n",
    "- **Retrievers**: Dễ dàng chuyển đổi giữa các phương pháp tìm kiếm (BM25, Vector Search, Hybrid Search)\n",
    "- **Evaluation framework**: Có sẵn công cụ đánh giá kết quả retrieval và generation\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SemanticChunker\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# Chuyển đổi giữa các kỹ thuật chunking\n",
    "char_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "semantic_splitter = SemanticChunker(embedding_function=embeddings)\n",
    "\n",
    "# Chuyển đổi giữa các retrievers\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "vector_retriever = vector_store.as_retriever()\n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever], weights=[0.5, 0.5])\n",
    "```\n",
    "[Source](https://python.langchain.com/docs/modules/data_connection/document_transformers/)\n",
    "\n",
    "## 2. LlamaIndex\n",
    "\n",
    "LlamaIndex là framework được thiết kế đặc biệt cho RAG với sự linh hoạt cao trong việc tùy chỉnh các thành phần.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Node parsers**: Nhiều cách phân đoạn tài liệu (sentence, fixed size, semantic)\n",
    "- **Retrievers**: Hỗ trợ nhiều retriever types và dễ dàng kết hợp chúng\n",
    "- **Query Engines**: Các engines khác nhau cho các use-cases khác nhau\n",
    "- **Evaluation**: Có sẵn bộ công cụ đánh giá toàn diện\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.retrievers import BM25Retriever, VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.evaluation import RetrievalEvaluator\n",
    "\n",
    "# Chuyển đổi giữa các node parsers\n",
    "sentence_parser = SentenceSplitter(chunk_size=1024)\n",
    "semantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n",
    "\n",
    "# Chuyển đổi giữa các retrievers\n",
    "vector_retriever = VectorStoreIndex.from_documents(docs).as_retriever()\n",
    "bm25_retriever = BM25Retriever.from_defaults(documents=docs)\n",
    "```\n",
    "[Source](https://docs.llamaindex.ai/en/stable/examples/node_parsers/semantic_chunking/)\n",
    "\n",
    "## 3. Haystack by Deepset\n",
    "\n",
    "Haystack xây dựng theo kiến trúc pipeline, cho phép dễ dàng thay đổi và kết hợp các thành phần.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Document Preprocessing**: Nhiều chiến lược chia nhỏ tài liệu\n",
    "- **Pipeline Architecture**: Thay đổi components không cần viết lại code\n",
    "- **Evaluation**: Công cụ đánh giá kết quả end-to-end và từng thành phần\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from haystack.nodes import PreProcessor, BM25Retriever, EmbeddingRetriever, JoinDocuments\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "# Các kỹ thuật chunking khác nhau\n",
    "preprocessor_fixed = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_overlap=10\n",
    ")\n",
    "\n",
    "preprocessor_sentence = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"sentence\",\n",
    "    split_overlap=3\n",
    ")\n",
    "\n",
    "# Chuyển đổi giữa các retriever\n",
    "bm25_retriever = BM25Retriever(document_store)\n",
    "embedding_retriever = EmbeddingRetriever(document_store, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pipeline hybrid search\n",
    "hybrid_pipeline = Pipeline()\n",
    "hybrid_pipeline.add_node(component=bm25_retriever, name=\"BM25Retriever\", inputs=[\"Query\"])\n",
    "hybrid_pipeline.add_node(component=embedding_retriever, name=\"EmbeddingRetriever\", inputs=[\"Query\"])\n",
    "hybrid_pipeline.add_node(component=JoinDocuments(join_mode=\"concatenate\"), name=\"JoinResults\", inputs=[\"BM25Retriever\", \"EmbeddingRetriever\"])\n",
    "```\n",
    "[Source](https://haystack.deepset.ai/components/preprocessor)\n",
    "\n",
    "## 4. Unstructured.io\n",
    "\n",
    "Unstructured.io tập trung vào xử lý tài liệu đa dạng và preprocessing, có thể tích hợp tốt với các frameworks khác.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Document processing**: Xử lý nhiều loại tài liệu (PDF, Word, Images)\n",
    "- **Chunking strategies**: Nhiều chiến lược chia nhỏ nội dung\n",
    "- **Integration**: Dễ dàng tích hợp với LangChain, LlamaIndex\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.partition.strategy import split_by_page, split_by_paragraph\n",
    "\n",
    "# Phân chia PDF theo trang\n",
    "elements_by_page = partition_pdf(\n",
    "    \"path/to/document.pdf\",\n",
    "    strategy=split_by_page()\n",
    ")\n",
    "\n",
    "# Phân chia PDF theo đoạn văn\n",
    "elements_by_paragraph = partition_pdf(\n",
    "    \"path/to/document.pdf\",\n",
    "    strategy=split_by_paragraph()\n",
    ")\n",
    "```\n",
    "[Source](https://unstructured-io.github.io/unstructured/core/partition.html)\n",
    "\n",
    "## 5. RAGAS\n",
    "\n",
    "RAGAS là framework đánh giá RAG, giúp so sánh hiệu suất của các phương pháp khác nhau.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Evaluators**: Đánh giá toàn diện về context precision/recall, faithfulness, answer relevance\n",
    "- **Benchmarking**: So sánh các kỹ thuật RAG khác nhau\n",
    "- **Integration**: Tích hợp với các frameworks phổ biến như LangChain, LlamaIndex\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Đánh giá hiệu suất của các kỹ thuật RAG khác nhau\n",
    "results = evaluate(\n",
    "    dataset=Dataset.from_dict(evaluation_data),\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy\n",
    "    ]\n",
    ")\n",
    "```\n",
    "[Source](https://github.com/explodinggradients/ragas)\n",
    "\n",
    "## 6. Griptape\n",
    "\n",
    "Griptape cung cấp framework với mô hình structure-first giúp tạo các ứng dụng RAG với logic rõ ràng.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Chunking Strategies**: Nhiều phương pháp phân đoạn tài liệu\n",
    "- **Pipelines**: Dễ dàng tạo và chỉnh sửa pipelines\n",
    "- **Evaluation**: Công cụ đánh giá cho từng bước trong pipeline\n",
    "\n",
    "### Ví dụ tích hợp:\n",
    "```python\n",
    "from griptape.structures import Pipeline\n",
    "from griptape.tasks import PromptTask\n",
    "from griptape.engines import VectorQueryEngine\n",
    "from griptape.loaders import PdfLoader\n",
    "from griptape.chunkers import ParagraphChunker, SemanticChunker\n",
    "\n",
    "# Loader và chunker\n",
    "pdf_loader = PdfLoader(\"path/to/document.pdf\")\n",
    "\n",
    "# Chuyển đổi giữa các chunkers\n",
    "paragraph_chunker = ParagraphChunker(max_tokens=500)\n",
    "semantic_chunker = SemanticChunker(max_tokens=500)\n",
    "\n",
    "# Tạo engines với các chunkers khác nhau\n",
    "vector_engine_1 = VectorQueryEngine(\n",
    "    artifacts=[pdf_loader.load(chunker=paragraph_chunker)]\n",
    ")\n",
    "\n",
    "vector_engine_2 = VectorQueryEngine(\n",
    "    artifacts=[pdf_loader.load(chunker=semantic_chunker)]\n",
    ")\n",
    "```\n",
    "[Source](https://github.com/griptape-ai/griptape)\n",
    "\n",
    "## So sánh các frameworks\n",
    "\n",
    "| Framework | Mức độ linh hoạt | Công cụ đánh giá | Chunking Strategies | Retrieval Methods | Khả năng tích hợp | Tài liệu |\n",
    "|-----------|------------------|------------------|---------------------|-------------------|-------------------|----------|\n",
    "| LangChain | ★★★★★ | ★★★★☆ | ★★★★★ | ★★★★★ | ★★★★★ | ★★★★★ |\n",
    "| LlamaIndex | ★★★★★ | ★★★★★ | ★★★★☆ | ★★★★★ | ★★★★☆ | ★★★★☆ |\n",
    "| Haystack | ★★★★☆ | ★★★★☆ | ★★★☆☆ | ★★★★☆ | ★★★★☆ | ★★★★☆ |\n",
    "| Unstructured.io | ★★★☆☆ | ★★☆☆☆ | ★★★★☆ | ★★☆☆☆ | ★★★★☆ | ★★★☆☆ |\n",
    "| RAGAS | ★★★☆☆ | ★★★★★ | ★★☆☆☆ | ★★☆☆☆ | ★★★★☆ | ★★★☆☆ |\n",
    "| Griptape | ★★★★☆ | ★★★☆☆ | ★★★☆☆ | ★★★☆☆ | ★★★☆☆ | ★★★☆☆ |\n",
    "\n",
    "## Kết luận và khuyến nghị\n",
    "\n",
    "Dựa trên yêu cầu của bạn về việc thử nghiệm nhanh với các kỹ thuật RAG khác nhau:\n",
    "\n",
    "1. **LangChain và LlamaIndex** là hai frameworks có mức độ linh hoạt cao nhất, phù hợp nhất cho việc thử nghiệm và chuyển đổi giữa các kỹ thuật khác nhau.\n",
    "\n",
    "2. **Kết hợp với RAGAS** để đánh giá hiệu suất của các kỹ thuật khác nhau sau khi thử nghiệm.\n",
    "\n",
    "3. **Haystack** cũng là một lựa chọn tốt nếu bạn muốn kiến trúc pipeline rõ ràng.\n",
    "\n",
    "4. Đối với việc đánh giá toàn diện các thành phần RAG, **LlamaIndex** có hệ thống evaluation khá đầy đủ và dễ sử dụng.\n",
    "\n",
    "LangChain có thể là lựa chọn tốt nhất nếu bạn muốn thử nghiệm nhanh với các kỹ thuật khác nhau trên cùng một tập dữ liệu, với khả năng tích hợp cao và API linh hoạt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GitHub - RUC-NLPIR/FlashRAG: ⚡FlashRAG: A Python Toolkit for Effi...\n",
    "\n",
    "thế những thằng như này thì sao nhỉ ??? \n",
    "---\n",
    "Tìm kiếm giải pháp\n",
    "1. Dễ dàng thử nghiệm nhanh các method, các kỹ thuật khác nhau trong từng component\n",
    "Với các hàm và code đơn giản do đưọc nhiều thư viện hỗ trợ sẵn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sau khi phân tích các framework RAG phổ biến đang có sẵn, tôi sẽ giúp bạn tìm hiểu cách chúng hỗ trợ việc chuyển qua lại nhanh giữa các kỹ thuật khác nhau trong RAG như Chunking, Retrieval và Generation.\n",
    "\n",
    "# So Sánh Các Framework RAG Hỗ Trợ Chuyển Đổi Nhanh Giữa Các Kỹ Thuật\n",
    "\n",
    "## 1. FlashRAG - Framework Linh Hoạt Dành Cho Nghiên Cứu\n",
    "\n",
    "[FlashRAG](https://github.com/RUC-NLPIR/FlashRAG) là framework mới được phát triển bởi nhóm nghiên cứu NLPIR của Đại học Renmin Trung Quốc, với ưu điểm nổi bật trong việc hỗ trợ thử nghiệm nhanh các kỹ thuật RAG khác nhau.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Kiến trúc cực kỳ mô-đun hóa**: Tách biệt các thành phần như retriever, reranker, generator và compressor\n",
    "- **Hỗ trợ đa dạng kỹ thuật chunking**: Tích hợp thư viện \"Chunkie\" hỗ trợ các phương pháp chia nhỏ tài liệu khác nhau (token, sentence, semantic)\n",
    "- **36 bộ dữ liệu được tiền xử lý**: Giúp nhanh chóng thử nghiệm và đánh giá các phương pháp khác nhau\n",
    "- **Hệ thống cấu hình đơn giản**: Sử dụng YAML hoặc Python dict để dễ dàng chuyển đổi giữa các kỹ thuật\n",
    "- **Giao diện người dùng trực quan**: FlashRAG-UI cho phép cấu hình nhanh và trực quan các thành phần\n",
    "\n",
    "### Các pipeline có sẵn:\n",
    "- **Sequential**: Thực thi tuần tự các bước trong quy trình RAG\n",
    "- **Conditional**: Thực hiện các đường dẫn khác nhau cho các loại truy vấn\n",
    "- **Branching**: Thực hiện nhiều đường dẫn song song\n",
    "- **Loop**: Lặp lại quá trình truy xuất và tạo dữ liệu\n",
    "\n",
    "### Đánh giá:\n",
    "✅ Cực kỳ linh hoạt cho việc thử nghiệm nhanh\n",
    "✅ Dễ dàng chuyển đổi giữa các kỹ thuật chunking và retrieval\n",
    "✅ Tích hợp tốt với nhiều công cụ đánh giá RAG\n",
    "✅ Giao diện trực quan giúp cấu hình nhanh chóng\n",
    "\n",
    "## 2. FastRAG - Hiệu Quả và Tối Ưu\n",
    "\n",
    "[FastRAG](https://github.com/IntelLabs/fastRAG) được phát triển bởi Intel Labs, tập trung vào hiệu suất và tối ưu hóa cho các pipeline RAG.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Tối ưu hóa phần cứng**: Tận dụng Intel extensions cho PyTorch, OpenVINO và Habana\n",
    "- **Hỗ trợ REPLUG**: Phương pháp truy xuất nâng cao cho mô hình ngôn ngữ\n",
    "- **ColBERT + PLAID**: Hệ thống truy xuất mật độ cao với độ trễ thấp\n",
    "- **Quantized Models**: Hỗ trợ mô hình lượng tử hóa để tăng tốc độ\n",
    "\n",
    "### Các kỹ thuật RAG hỗ trợ:\n",
    "- Dense retrieval với ColBERT v2\n",
    "- Fusion-In-Decoder (FiD) cho generation\n",
    "- Quantized bi-encoder models\n",
    "\n",
    "### Đánh giá:\n",
    "✅ Hiệu năng cao, tối ưu cho phần cứng Intel\n",
    "✅ Tích hợp tốt với Haystack\n",
    "✅ Hỗ trợ nhiều phương pháp truy xuất và tổng hợp\n",
    "❌ Ít tập trung vào chunking so với FlashRAG\n",
    "\n",
    "## 3. LazyGraphRAG - Tiếp Cận Dựa Trên Đồ Thị\n",
    "\n",
    "[LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/) là framework mới từ Microsoft Research, có cách tiếp cận RAG dựa trên đồ thị.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Không cần tóm tắt trước**: Loại bỏ chi phí tiền xử lý đắt đỏ\n",
    "- **Đánh giá lười biếng**: Sử dụng chiến lược tìm kiếm best-first và breadth-first\n",
    "- **Cấu trúc đồ thị linh hoạt**: Tự động tìm mối quan hệ giữa các đoạn văn\n",
    "- **Xử lý truy vấn cục bộ và toàn cục**: Xử lý hiệu quả cả hai loại truy vấn\n",
    "\n",
    "### Đánh giá:\n",
    "✅ Giảm chi phí chỉ mục hóa, tăng khả năng mở rộng\n",
    "✅ Chất lượng kết quả cao cho cả truy vấn cục bộ và toàn cục\n",
    "❌ Ít tập trung vào việc chuyển đổi giữa các kỹ thuật chunking\n",
    "\n",
    "## 4. LangChain - Hệ Sinh Thái Mở Rộng\n",
    "\n",
    "[LangChain](https://www.langchain.com/) là một trong những framework RAG phổ biến nhất, cung cấp hệ sinh thái phong phú cho các ứng dụng LLM.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Document Transformers**: Nhiều phương pháp chunking (character, token, semantic, recursive)\n",
    "- **Retrievers đa dạng**: Dễ dàng chuyển đổi giữa các phương pháp tìm kiếm (BM25, Vector, Hybrid)\n",
    "- **Evaluation framework**: Đánh giá kết quả retrieval và generation\n",
    "- **Chains và Agents**: Hỗ trợ tạo chuỗi các thao tác và agent tự chủ\n",
    "\n",
    "### Đánh giá:\n",
    "✅ Hệ sinh thái phong phú với nhiều tích hợp\n",
    "✅ Linh hoạt cao trong việc chuyển đổi giữa các kỹ thuật\n",
    "✅ Cộng đồng lớn và tài liệu phong phú\n",
    "❌ Có thể phức tạp hơn đối với người mới bắt đầu\n",
    "\n",
    "## 5. LlamaIndex - Tập Trung Vào Dữ Liệu\n",
    "\n",
    "[LlamaIndex](https://www.llamaindex.ai/) là framework tập trung vào việc kết nối LLMs với các nguồn dữ liệu cá nhân và doanh nghiệp.\n",
    "\n",
    "### Tính năng nổi bật:\n",
    "- **Node parsers**: Nhiều cách phân đoạn tài liệu (sentence, fixed size, semantic)\n",
    "- **Retrievers đa dạng**: Hỗ trợ nhiều loại retriever và dễ dàng kết hợp\n",
    "- **Query Engines**: Các engines khác nhau cho các use-cases khác nhau\n",
    "- **Đánh giá toàn diện**: Công cụ đánh giá cho từng bước trong pipeline\n",
    "\n",
    "### Đánh giá:\n",
    "✅ Tập trung vào truy vấn dữ liệu có cấu trúc và không cấu trúc\n",
    "✅ Tích hợp mạnh mẽ với các nguồn dữ liệu doanh nghiệp\n",
    "✅ Hỗ trợ nhiều phương pháp chunking và indexing\n",
    "\n",
    "## Bảng So Sánh Chi Tiết\n",
    "\n",
    "| Tính năng | FlashRAG | FastRAG | LazyGraphRAG | LangChain | LlamaIndex |\n",
    "|-----------|----------|---------|--------------|-----------|------------|\n",
    "| **Chunking Strategies** |\n",
    "| Character/Token Chunking | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| Semantic Chunking | ✅ | ⚠️ | ✅ | ✅ | ✅ |\n",
    "| Recursive Chunking | ✅ | ❌ | ❌ | ✅ | ⚠️ |\n",
    "| **Retrieval Methods** |\n",
    "| Dense Retrieval | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| Sparse Retrieval (BM25) | ✅ | ✅ | ⚠️ | ✅ | ✅ |\n",
    "| Hybrid Search | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| Graph-based Retrieval | ⚠️ | ❌ | ✅ | ⚠️ | ⚠️ |\n",
    "| **Generation Approaches** |\n",
    "| Standard RAG | ✅ | ✅ | ✅ | ✅ | ✅ |\n",
    "| Iterative RAG | ✅ | ⚠️ | ✅ | ✅ | ✅ |\n",
    "| Fusion-in-Decoder | ⚠️ | ✅ | ❌ | ⚠️ | ⚠️ |\n",
    "| **Evaluation** |\n",
    "| Built-in Evaluation | ✅ | ⚠️ | ⚠️ | ✅ | ✅ |\n",
    "| Integration with RAGAS | ✅ | ❌ | ❌ | ✅ | ✅ |\n",
    "| **Chuyển Đổi Giữa Kỹ Thuật** |\n",
    "| Dễ dàng chuyển kỹ thuật chunking | ✅✅ | ⚠️ | ⚠️ | ✅ | ✅ |\n",
    "| Dễ dàng chuyển kỹ thuật retrieval | ✅✅ | ✅ | ⚠️ | ✅ | ✅ |\n",
    "| Dễ dàng chuyển kỹ thuật generation | ✅✅ | ✅ | ⚠️ | ✅ | ✅ |\n",
    "\n",
    "✅✅: Xuất sắc, ✅: Tốt, ⚠️: Trung bình/Hạn chế, ❌: Không hỗ trợ/Yếu\n",
    "\n",
    "## Khuyến Nghị Cho Use Case Của Bạn\n",
    "\n",
    "Dựa trên yêu cầu của bạn về việc cần một framework hỗ trợ chuyển qua lại nhanh giữa các kỹ thuật RAG khác nhau như Chunking, Retrieval, và Generation, tôi đề xuất:\n",
    "\n",
    "### 1. FlashRAG\n",
    "\n",
    "**Lý do**: \n",
    "- FlashRAG được thiết kế đặc biệt cho mục đích nghiên cứu và thử nghiệm RAG\n",
    "- Hỗ trợ nhanh chóng chuyển đổi giữa các kỹ thuật chunking khác nhau thông qua thư viện Chunkie\n",
    "- Cung cấp UI trực quan để cấu hình và thay đổi các thành phần\n",
    "- Đi kèm 36 dataset đã được tiền xử lý để đánh giá nhanh các phương pháp khác nhau\n",
    "- Dễ dàng chuyển đổi giữa các phương pháp Retrieval (BM25, E5, BGE, v.v.)\n",
    "\n",
    "```python\n",
    "# Ví dụ với FlashRAG - Chuyển đổi nhanh giữa các chunking methods\n",
    "from flashrag.config import Config\n",
    "\n",
    "# Cấu hình cho semantic chunking\n",
    "semantic_config = Config(\n",
    "    config_dict={\n",
    "        \"chunker\": {\n",
    "            \"type\": \"semantic\",\n",
    "            \"params\": {\n",
    "                \"max_tokens\": 500\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Cấu hình cho token chunking\n",
    "token_config = Config(\n",
    "    config_dict={\n",
    "        \"chunker\": {\n",
    "            \"type\": \"token\",\n",
    "            \"params\": {\n",
    "                \"chunk_size\": 1000,\n",
    "                \"chunk_overlap\": 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chuyển đổi giữa các retriever\n",
    "bm25_config = Config(\n",
    "    config_dict={\n",
    "        \"retriever\": {\n",
    "            \"type\": \"bm25\",\n",
    "            \"params\": {\n",
    "                \"top_k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "dense_config = Config(\n",
    "    config_dict={\n",
    "        \"retriever\": {\n",
    "            \"type\": \"e5\",\n",
    "            \"params\": {\n",
    "                \"top_k\": 5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### 2. LangChain\n",
    "\n",
    "**Lý do**:\n",
    "- Hệ sinh thái phong phú với nhiều tích hợp và mô-đun có sẵn\n",
    "- Linh hoạt trong việc chuyển đổi giữa các kỹ thuật thông qua API nhất quán\n",
    "- Cộng đồng lớn và nhiều tài liệu, ví dụ hỗ trợ\n",
    "\n",
    "```python\n",
    "# Ví dụ LangChain - Chuyển đổi giữa các text splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Character-based chunking\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Recursive chunking\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Semantic chunking\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Chuyển đổi giữa các retriever\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "\n",
    "# Vector Retriever\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "vector_retriever = vector_store.as_retriever()\n",
    "\n",
    "# Hybrid Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. LlamaIndex\n",
    "\n",
    "**Lý do**:\n",
    "- Tập trung vào truy xuất dữ liệu và tích hợp với nhiều nguồn dữ liệu\n",
    "- Hỗ trợ nhiều node parsers cho các kỹ thuật chunking khác nhau\n",
    "- Cung cấp query engines đa dạng cho các use-cases khác nhau\n",
    "\n",
    "## Kết Luận\n",
    "\n",
    "Đối với nhu cầu \"thử nghiệm nhanh và chuyển đổi giữa các kỹ thuật RAG khác nhau\", **FlashRAG** nổi bật như một giải pháp tối ưu. Framework này được thiết kế đặc biệt cho mục đích nghiên cứu và thử nghiệm, với khả năng dễ dàng thay đổi các thành phần trong pipeline RAG.\n",
    "\n",
    "Nếu bạn cần một hệ sinh thái phong phú hơn với nhiều tích hợp, **LangChain** và **LlamaIndex** cũng là những lựa chọn đáng cân nhắc. Với những yêu cầu tập trung vào hiệu năng và tối ưu hóa, **FastRAG** sẽ là lựa chọn phù hợp hơn.\n",
    "\n",
    "Mỗi framework có ưu điểm riêng, nhưng FlashRAG dường như đáp ứng tốt nhất nhu cầu cụ thể của bạn về việc thử nghiệm nhanh các kỹ thuật chunking, retrieval và generation khác nhau trên cùng một tập dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# có cái nào ngon hơn flashRAG ko "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.firecrawl.dev/blog/best-open-source-rag-frameworks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là phân tích về các frameworks RAG hỗ trợ chuyển đổi nhanh giữa các kỹ thuật khác nhau:\n",
    "\n",
    "# Các Framework RAG Hỗ Trợ Chuyển Đổi Nhanh Giữa Các Kỹ Thuật\n",
    "\n",
    "Để đáp ứng nhu cầu thử nghiệm và chuyển đổi nhanh giữa các kỹ thuật RAG (Retrieval Augmented Generation) như Chunking, Retrieval và Generation, dưới đây là phân tích chi tiết về các frameworks hiện có và khả năng hỗ trợ của chúng.\n",
    "\n",
    "## 1. FlashRAG - Framework Chuyên Dụng Cho Nghiên Cứu RAG\n",
    "\n",
    "[FlashRAG](https://github.com/RUC-NLPIR/FlashRAG) là toolkit Python được phát triển bởi nhóm nghiên cứu NLPIR của Đại học Renmin Trung Quốc, với ưu điểm nổi bật trong việc hỗ trợ thử nghiệm nhanh các kỹ thuật RAG khác nhau.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Kiến trúc cực kỳ mô-đun hóa:** Tách biệt các thành phần như retriever, reranker, generator và compressor\n",
    "- **36 bộ dữ liệu được tiền xử lý:** Giúp nhanh chóng thử nghiệm và đánh giá các phương pháp khác nhau\n",
    "- **Hệ thống cấu hình đơn giản:** Sử dụng YAML hoặc Python dict để dễ dàng chuyển đổi giữa các kỹ thuật\n",
    "- **Giao diện người dùng trực quan:** FlashRAG-UI cho phép cấu hình nhanh và trực quan các thành phần\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- Chuyển đổi giữa các phương pháp chunking khác nhau thông qua thư viện Chunkie\n",
    "- Dễ dàng thay đổi retriever từ BM25 sang nhiều loại vector retrievers\n",
    "\n",
    "**Ví dụ:**\n",
    "```python\n",
    "# Chuyển đổi nhanh giữa các chunking methods\n",
    "from flashrag.config import Config\n",
    "\n",
    "# Cấu hình cho semantic chunking\n",
    "semantic_config = Config(\n",
    "    config_dict={\n",
    "        \"chunker\": {\n",
    "            \"type\": \"semantic\",\n",
    "            \"params\": {\n",
    "                \"max_tokens\": 500\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Chuyển đổi sang token chunking\n",
    "token_config = Config(\n",
    "    config_dict={\n",
    "        \"chunker\": {\n",
    "            \"type\": \"token\",\n",
    "            \"params\": {\n",
    "                \"chunk_size\": 1000,\n",
    "                \"chunk_overlap\": 100\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "## 2. LangChain - Hệ Sinh Thái Linh Hoạt\n",
    "\n",
    "[LangChain](https://github.com/langchain-ai/langchain) là một trong những frameworks RAG phổ biến nhất, cung cấp hệ sinh thái mạnh mẽ và linh hoạt.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Document Transformers:** Nhiều phương pháp chunking (character, token, semantic, recursive)\n",
    "- **Retrievers đa dạng:** Dễ dàng chuyển đổi giữa BM25, vector search, và hybrid search\n",
    "- **Evaluation framework:** Công cụ đánh giá kết quả retrieval và generation\n",
    "- **Chains và Agents:** Hỗ trợ tạo chuỗi các thao tác và agent tự chủ\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- API nhất quán cho việc chuyển đổi giữa các text splitters\n",
    "- Hỗ trợ kết hợp các retrievers với trọng số linh hoạt\n",
    "- Tài liệu phong phú với nhiều ví dụ ứng dụng\n",
    "\n",
    "**Ví dụ:**\n",
    "```python\n",
    "# Chuyển đổi giữa các text splitter\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, SemanticChunker\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Character-based chunking\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Recursive chunking\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Semantic chunking\n",
    "embeddings = OpenAIEmbeddings()\n",
    "semantic_splitter = SemanticChunker(embeddings=embeddings)\n",
    "\n",
    "# Chuyển đổi giữa các retriever\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "\n",
    "# Vector Retriever\n",
    "vector_store = Chroma.from_documents(documents, embeddings)\n",
    "vector_retriever = vector_store.as_retriever()\n",
    "\n",
    "# Hybrid Retriever\n",
    "hybrid_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "```\n",
    "\n",
    "## 3. LlamaIndex - Tập Trung Vào Dữ Liệu\n",
    "\n",
    "[LlamaIndex](https://github.com/run-llama/llama_index) là framework được thiết kế đặc biệt cho RAG với khả năng linh hoạt cao trong việc tùy chỉnh các thành phần.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Node parsers:** Nhiều cách phân đoạn tài liệu (sentence, fixed size, semantic)\n",
    "- **Retrievers đa dạng:** Hỗ trợ nhiều loại retrievers và dễ dàng kết hợp\n",
    "- **Query Engines:** Các engines khác nhau cho các use-cases khác nhau\n",
    "- **Evaluation:** Có sẵn bộ công cụ đánh giá toàn diện\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- API đơn giản cho việc chuyển đổi giữa các node parsers\n",
    "- Dễ dàng thay đổi retriever và query engine\n",
    "\n",
    "**Ví dụ:**\n",
    "```python\n",
    "from llama_index import ServiceContext, VectorStoreIndex\n",
    "from llama_index.node_parser import SentenceSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.retrievers import BM25Retriever, VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.evaluation import RetrievalEvaluator\n",
    "\n",
    "# Chuyển đổi giữa các node parsers\n",
    "sentence_parser = SentenceSplitter(chunk_size=1024)\n",
    "semantic_parser = SemanticSplitterNodeParser(embed_model=embed_model)\n",
    "\n",
    "# Chuyển đổi giữa các retrievers\n",
    "vector_retriever = VectorStoreIndex.from_documents(docs).as_retriever()\n",
    "bm25_retriever = BM25Retriever.from_defaults(documents=docs)\n",
    "```\n",
    "\n",
    "## 4. Haystack by Deepset\n",
    "\n",
    "[Haystack](https://github.com/deepset-ai/haystack) được xây dựng theo kiến trúc pipeline, cho phép dễ dàng thay đổi và kết hợp các thành phần.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Document Preprocessing:** Nhiều chiến lược chia nhỏ tài liệu\n",
    "- **Pipeline Architecture:** Thay đổi components không cần viết lại code\n",
    "- **Evaluation:** Công cụ đánh giá kết quả end-to-end và từng thành phần\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- API pipeline rõ ràng để thay đổi các nodes\n",
    "- Dễ dàng định nghĩa và kết hợp các retrievers\n",
    "\n",
    "**Ví dụ:**\n",
    "```python\n",
    "from haystack.nodes import PreProcessor, BM25Retriever, EmbeddingRetriever, JoinDocuments\n",
    "from haystack.pipelines import Pipeline\n",
    "\n",
    "# Các kỹ thuật chunking khác nhau\n",
    "preprocessor_fixed = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"word\",\n",
    "    split_length=100,\n",
    "    split_overlap=10\n",
    ")\n",
    "\n",
    "preprocessor_sentence = PreProcessor(\n",
    "    clean_empty_lines=True,\n",
    "    split_by=\"sentence\",\n",
    "    split_overlap=3\n",
    ")\n",
    "\n",
    "# Chuyển đổi giữa các retriever\n",
    "bm25_retriever = BM25Retriever(document_store)\n",
    "embedding_retriever = EmbeddingRetriever(document_store, embedding_model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Pipeline hybrid search\n",
    "hybrid_pipeline = Pipeline()\n",
    "hybrid_pipeline.add_node(component=bm25_retriever, name=\"BM25Retriever\", inputs=[\"Query\"])\n",
    "hybrid_pipeline.add_node(component=embedding_retriever, name=\"EmbeddingRetriever\", inputs=[\"Query\"])\n",
    "hybrid_pipeline.add_node(component=JoinDocuments(join_mode=\"concatenate\"), name=\"JoinResults\", inputs=[\"BM25Retriever\", \"EmbeddingRetriever\"])\n",
    "```\n",
    "\n",
    "## 5. LazyGraphRAG - Tiếp Cận Dựa Trên Đồ Thị\n",
    "\n",
    "[LazyGraphRAG](https://www.microsoft.com/en-us/research/blog/lazygraphrag-setting-a-new-standard-for-quality-and-cost/) là framework mới từ Microsoft Research, có cách tiếp cận RAG dựa trên đồ thị.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Không cần tóm tắt trước:** Loại bỏ chi phí tiền xử lý đắt đỏ\n",
    "- **Đánh giá lười biếng:** Sử dụng chiến lược tìm kiếm best-first và breadth-first\n",
    "- **Cấu trúc đồ thị linh hoạt:** Tự động tìm mối quan hệ giữa các đoạn văn\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- Kết hợp tìm kiếm vector và đồ thị một cách linh hoạt\n",
    "- Cân bằng giữa truy vấn cục bộ và toàn cục\n",
    "\n",
    "LazyGraphRAG đặc biệt hiệu quả cho việc giảm chi phí chỉ mục hóa, tuy nhiên không tập trung nhiều vào việc thử nghiệm các kỹ thuật chunking khác nhau như yêu cầu của bạn.\n",
    "\n",
    "## 6. RAGFlow - Hệ Thống End-to-End\n",
    "\n",
    "[RAGFlow](https://github.com/infiniflow/ragflow) là framework RAG được phát triển với trọng tâm vào việc xử lý tài liệu phức tạp.\n",
    "\n",
    "**Tính năng nổi bật:**\n",
    "- **Deep document understanding:** Trích xuất văn bản, bảng, cấu trúc từ tài liệu phức tạp\n",
    "- **Visual web interface:** Giao diện thân thiện cho quản lý tài liệu và quy trình RAG\n",
    "- **GraphRAG support:** Tạo đồ thị kiến thức từ tài liệu để truy vấn ngữ cảnh tốt hơn\n",
    "\n",
    "**Khả năng chuyển đổi kỹ thuật:**\n",
    "- Hỗ trợ nhiều loại embedding models khác nhau\n",
    "- Linh hoạt trong việc lưu trữ và truy xuất dữ liệu với Elasticsearch hoặc Infinity\n",
    "\n",
    "## So Sánh Các Framework\n",
    "\n",
    "| Framework | Chunking Methods | Retrieval Methods | Generation Approaches | Evaluation | Dễ dàng chuyển đổi | GitHub Stars |\n",
    "|-----------|------------------|-------------------|------------------------|------------|---------------------|--------------|\n",
    "| FlashRAG | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | 2.1k |\n",
    "| LangChain | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 105k |\n",
    "| LlamaIndex | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 40.8k |\n",
    "| Haystack | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | 20.2k |\n",
    "| LazyGraphRAG | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | - |\n",
    "| RAGFlow | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ | 48.5k |\n",
    "\n",
    "## Frameworks Mới Nổi Khác\n",
    "\n",
    "Ngoài các frameworks đã nêu, còn một số frameworks mới nổi đáng chú ý:\n",
    "\n",
    "1. **LightRAG:** Framework nhẹ tập trung vào hiệu suất, với thiết kế đơn giản và dễ triển khai.\n",
    "\n",
    "2. **DSPy:** Framework của Stanford NLP tập trung vào tối ưu hóa prompt tự động, tương thích với nhiều hệ thống RAG.\n",
    "\n",
    "3. **R2R (RAG to Riches):** Hệ thống RAG tiên tiến với khả năng reasoning sâu, hỗ trợ xử lý đa dạng định dạng dữ liệu.\n",
    "\n",
    "4. **Ragatouille:** Tập trung vào retrieval với kỹ thuật \"late-interaction\" dựa trên ColBERT, cho độ chính xác cao hơn.\n",
    "\n",
    "## Khuyến Nghị Cho Use Case Của Bạn\n",
    "\n",
    "Dựa trên yêu cầu của bạn về việc cần thử nghiệm nhanh với các kỹ thuật Chunking, Retrieval và Generation khác nhau:\n",
    "\n",
    "### 1. FlashRAG\n",
    "**Điểm mạnh:** \n",
    "- Được thiết kế đặc biệt cho việc nghiên cứu và thử nghiệm RAG\n",
    "- Hỗ trợ 36 bộ dữ liệu được tiền xử lý\n",
    "- Giao diện người dùng trực quan để cấu hình nhanh\n",
    "- Cực kỳ linh hoạt trong việc thay đổi các thành phần\n",
    "\n",
    "### 2. LangChain\n",
    "**Điểm mạnh:**\n",
    "- Hệ sinh thái phong phú với cộng đồng lớn\n",
    "- API nhất quán cho việc chuyển đổi giữa các kỹ thuật\n",
    "- Tích hợp tốt với nhiều công cụ bên ngoài\n",
    "- Tài liệu và ví dụ phong phú\n",
    "\n",
    "### 3. LlamaIndex\n",
    "**Điểm mạnh:**\n",
    "- Tập trung vào việc kết nối LLM với dữ liệu cá nhân\n",
    "- Kiến trúc module rõ ràng dễ dàng thay đổi các thành phần\n",
    "- Hỗ trợ nhiều loại dữ liệu đầu vào\n",
    "- Công cụ đánh giá toàn diện\n",
    "\n",
    "## Kết Luận\n",
    "\n",
    "FlashRAG nổi bật như giải pháp tối ưu cho yêu cầu của bạn về việc thử nghiệm nhanh và chuyển đổi giữa các kỹ thuật RAG khác nhau. Dù có số lượng GitHub Stars thấp hơn các framework khác, nhưng nó được thiết kế đặc biệt cho mục đích nghiên cứu và thử nghiệm, với khả năng dễ dàng thay đổi các thành phần trong pipeline RAG.\n",
    "\n",
    "Tuy nhiên, nếu bạn cần một hệ sinh thái phong phú hơn với nhiều tích hợp sẵn và cộng đồng lớn, LangChain và LlamaIndex cũng là những lựa chọn đáng cân nhắc. Đặc biệt, LangChain có thể là giải pháp cân bằng tốt giữa tính linh hoạt và sự phổ biến, với tài liệu phong phú giúp bạn nhanh chóng làm quen và triển khai.\n",
    "\n",
    "Cần lưu ý rằng không có framework nào là hoàn hảo cho mọi use case. Việc lựa chọn phụ thuộc vào yêu cầu cụ thể, nguồn lực kỹ thuật và thời gian phát triển của dự án."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
