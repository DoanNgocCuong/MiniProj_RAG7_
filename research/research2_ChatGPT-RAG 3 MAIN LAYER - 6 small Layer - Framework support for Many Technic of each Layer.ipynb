{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pha offline \n",
    "- Chunking -> Embedding -> Save Vector DB. \n",
    "# 2. Pha Online: \n",
    "- Retrieval\n",
    "- Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Data Ingestion Layer\n",
    "   ├── Document Loaders (PDF, HTML, Text, etc.)\n",
    "   ├── Text Extraction\n",
    "   └── Data Cleaning\n",
    "\n",
    "2. Chunking Layer\n",
    "   ├── Strategy Selector (Size-based, Semantic, Recursive)\n",
    "   └── Chunking Configuration\n",
    "\n",
    "3. Embedding Layer\n",
    "   ├── Model Selector (OpenAI, HuggingFace, etc.)\n",
    "   └── Embedding Configuration\n",
    "\n",
    "4. Retrieval Layer\n",
    "   ├── Retriever Selector (BM25, Vector, Hybrid)\n",
    "   ├── Reranking Integration\n",
    "   └── Retrieval Configuration\n",
    "\n",
    "5. Generation Layer\n",
    "   ├── LLM Selector\n",
    "   ├── Prompt Templates\n",
    "   └── Response Generation\n",
    "\n",
    "6. Evaluation Layer\n",
    "   ├── Metrics (Precision, Recall, Relevance)\n",
    "   ├── Benchmarking\n",
    "   └── Performance Tracking\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT là file PDF \n",
    "---\n",
    "output là luồng RAG và chạy benchmark được với các kỹ thuật khác nhau trong 6 phần \n",
    "\n",
    "```\n",
    "1. Data Ingestion Layer\n",
    "   ├── Document Loaders (PDF, HTML, Text, etc.)\n",
    "   ├── Text Extraction\n",
    "   └── Data Cleaning\n",
    "\n",
    "2. Chunking Layer\n",
    "   ├── Strategy Selector (Size-based, Semantic, Recursive)\n",
    "   └── Chunking Configuration\n",
    "\n",
    "3. Embedding Layer\n",
    "   ├── Model Selector (OpenAI, HuggingFace, etc.)\n",
    "   └── Embedding Configuration\n",
    "\n",
    "4. Retrieval Layer\n",
    "   ├── Retriever Selector (BM25, Vector, Hybrid)\n",
    "   ├── Reranking Integration\n",
    "   └── Retrieval Configuration\n",
    "\n",
    "5. Generation Layer\n",
    "   ├── LLM Selector\n",
    "   ├── Prompt Templates\n",
    "   └── Response Generation\n",
    "\n",
    "6. Evaluation Layer\n",
    "   ├── Metrics (Precision, Recall, Relevance)\n",
    "   ├── Benchmarking\n",
    "   └── Performance Tracking\n",
    "```\n",
    "\n",
    "thì FLASH RAG hỗ trợ ko ý "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Code: Ban đầu tạo 6 folder, code a Minh refactor thành 5 folder, xong fix mãi ko được => Genspark chia ra cho 6 phần. Input nó vào cursorx (xoá hết code cũ đi), và kết quả quá ngon. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Tổng hợp về Kiến trúc RAG (Retrieval-Augmented Generation) với LangChain\n",
    "\n",
    "Dựa trên tìm hiểu từ các nguồn tài liệu và mã nguồn của cộng đồng, dưới đây là chi tiết về cách triển khai kiến trúc RAG (Retrieval-Augmented Generation) sử dụng LangChain theo từng layer như bạn đã đề cập:\n",
    "\n",
    "## 1. Data Ingestion Layer (Lớp Nạp Dữ Liệu)\n",
    "\n",
    "Lớp này chịu trách nhiệm tải dữ liệu từ các nguồn khác nhau và chuyển đổi thành các đối tượng Document - định dạng tiêu chuẩn của LangChain.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **Document Loaders**: Class để tải dữ liệu từ nhiều nguồn khác nhau\n",
    "- **Data Cleaning**: Tiền xử lý và làm sạch dữ liệu\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, CSVLoader\n",
    "import bs4\n",
    "\n",
    "# Tải dữ liệu từ trang web\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# Tải dữ liệu từ PDF\n",
    "pdf_loader = PyPDFLoader(\"path/to/document.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# Tải dữ liệu từ CSV\n",
    "csv_loader = CSVLoader(\"path/to/data.csv\")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "# Kết hợp tất cả tài liệu\n",
    "all_docs = web_docs + pdf_docs + csv_docs\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Hỗ trợ nhiều loại nguồn dữ liệu: PDF, HTML, Text, Office docs, Markdown, Notion, v.v.\n",
    "- Tích hợp với nhiều API và cơ sở dữ liệu\n",
    "- Khả năng xử lý nhiều định dạng đầu vào và chuyển đổi thành Document chuẩn\n",
    "\n",
    "## 2. Chunking Layer (Lớp Chia Nhỏ)\n",
    "\n",
    "Lớp này chịu trách nhiệm phân chia các tài liệu dài thành các đoạn nhỏ hơn, dễ dàng hơn cho việc embedding và retrieval.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **Text Splitters**: Class để chia nhỏ tài liệu với nhiều chiến lược khác nhau\n",
    "- **Chunking Strategies**: Các chiến lược chia nhỏ như size-based, semantic, recursive\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Phương pháp chia nhỏ dựa trên độ dài chuỗi ký tự\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Số ký tự tối đa cho mỗi chunk\n",
    "    chunk_overlap=200,  # Số ký tự chồng lấp giữa các chunk\n",
    "    add_start_index=True,  # Theo dõi vị trí bắt đầu trong tài liệu gốc\n",
    ")\n",
    "text_chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Phương pháp chia nhỏ dựa trên ngữ nghĩa\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "semantic_chunks = semantic_splitter.split_documents(all_docs)\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Chia nhỏ dựa trên kích thước (size-based chunking)\n",
    "- Chia nhỏ theo ngữ nghĩa (semantic chunking)\n",
    "- Chia nhỏ đệ quy (recursive chunking)\n",
    "- Chia nhỏ theo đoạn văn hoặc cấu trúc tài liệu\n",
    "- Proposition chunking (tách thành các mệnh đề hoặc câu có nghĩa)\n",
    "\n",
    "## 3. Embedding Layer (Lớp Nhúng)\n",
    "\n",
    "Lớp này chịu trách nhiệm chuyển đổi các đoạn văn bản thành vector số (embeddings) để hỗ trợ tìm kiếm ngữ nghĩa.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **Embedding Models**: Mô hình để chuyển đổi text thành vector\n",
    "- **Vector Stores**: Cơ sở dữ liệu lưu trữ vector\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Khởi tạo embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# Tạo vector store trong bộ nhớ\n",
    "memory_vector_store = InMemoryVectorStore(embeddings)\n",
    "document_ids = memory_vector_store.add_documents(documents=text_chunks)\n",
    "\n",
    "# Hoặc sử dụng Chroma DB để lưu trữ\n",
    "db = Chroma.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Hoặc sử dụng FAISS (Facebook AI Similarity Search)\n",
    "faiss_db = FAISS.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Hỗ trợ nhiều mô hình embedding: OpenAI, HuggingFace, SentenceTransformers, v.v.\n",
    "- Tích hợp với nhiều vector database: Chroma, FAISS, Pinecone, Weaviate, Milvus, v.v.\n",
    "- Cấu hình và tối ưu hóa các tham số embedding\n",
    "\n",
    "## 4. Retrieval Layer (Lớp Truy Xuất)\n",
    "\n",
    "Lớp này chịu trách nhiệm truy xuất các đoạn văn bản liên quan nhất từ vector store dựa trên các truy vấn đầu vào.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **Retrievers**: Class truy xuất tài liệu từ vector stores\n",
    "- **Retrieval Strategies**: Các chiến lược truy xuất như BM25, Vector Search, Hybrid Search\n",
    "- **Rerankers**: Class sắp xếp lại các kết quả để tối ưu hóa độ chính xác\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_community.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Vector retriever cơ bản\n",
    "vector_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# BM25 Retriever (từ khóa)\n",
    "bm25_retriever = BM25Retriever.from_documents(text_chunks)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "# Ensemble Retriever (kết hợp nhiều loại)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever], \n",
    "    weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "# Contextual Compression Retriever (nén và lọc kết quả)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_retriever\n",
    ")\n",
    "\n",
    "# Sử dụng retriever để lấy tài liệu liên quan\n",
    "query = \"What is Task Decomposition?\"\n",
    "docs = vector_retriever.invoke(query)\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Truy xuất ngữ nghĩa (Semantic Retrieval)\n",
    "- Reranking kết quả bằng mô hình cross-encoder\n",
    "- Hybrid Search (kết hợp nhiều phương pháp tìm kiếm)\n",
    "- Self-query (tự động tạo filter từ câu hỏi người dùng)\n",
    "- Multi-query retrieval (tạo nhiều truy vấn khác nhau từ một câu hỏi)\n",
    "- Query transformation (biến đổi câu hỏi để tối ưu kết quả tìm kiếm)\n",
    "\n",
    "## 5. Generation Layer (Lớp Sinh Nội Dung)\n",
    "\n",
    "Lớp này chịu trách nhiệm sinh nội dung dựa trên thông tin truy xuất được và câu hỏi của người dùng.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **LLMs/ChatModels**: Mô hình ngôn ngữ lớn để sinh nội dung\n",
    "- **Prompt Templates**: Mẫu lời nhắc để hướng dẫn mô hình sinh nội dung\n",
    "- **Chains/Graphs**: Luồng xử lý để kết hợp truy xuất và sinh nội dung\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Khởi tạo mô hình ngôn ngữ\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Sử dụng prompt có sẵn từ hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Định nghĩa state cho ứng dụng\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Định nghĩa các bước xử lý\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Tạo graph và compile\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# Sử dụng graph để tạo câu trả lời\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Tùy chỉnh prompt templates theo nhiều chiến lược khác nhau\n",
    "- Contextual generation (sinh nội dung dựa trên ngữ cảnh)\n",
    "- Iterative refinement (tinh chỉnh nội dung qua nhiều lần lặp)\n",
    "- Bổ sung meta-information vào câu trả lời\n",
    "- Tích hợp với nhiều mô hình ngôn ngữ khác nhau: OpenAI, Anthropic, Google, v.v.\n",
    "\n",
    "## 6. Evaluation Layer (Lớp Đánh Giá)\n",
    "\n",
    "Lớp này chịu trách nhiệm đánh giá chất lượng của hệ thống RAG, từ retrieval đến generation.\n",
    "\n",
    "### Các thành phần chính:\n",
    "- **Evaluators**: Đánh giá các khía cạnh khác nhau của hệ thống\n",
    "- **Metrics**: Các thước đo để đánh giá hiệu suất\n",
    "- **Benchmarking**: So sánh hiệu suất giữa các cấu hình khác nhau\n",
    "- **LangSmith**: Công cụ theo dõi và đánh giá các ứng dụng RAG\n",
    "\n",
    "### Ví dụ code triển khai:\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "# Khởi tạo client LangSmith\n",
    "client = Client()\n",
    "\n",
    "# Tạo hàm đánh giá tính chính xác\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Đánh giá tính chính xác của câu trả lời\"\"\"\n",
    "    # Logic đánh giá\n",
    "    return True/False\n",
    "\n",
    "# Đánh giá sự phù hợp của câu trả lời với câu hỏi\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Đánh giá sự phù hợp của câu trả lời\"\"\"\n",
    "    # Logic đánh giá\n",
    "    return True/False\n",
    "\n",
    "# Đánh giá tính grounded (dựa trên thông tin được truy xuất)\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Đánh giá tính grounded của câu trả lời\"\"\"\n",
    "    # Logic đánh giá\n",
    "    return True/False\n",
    "\n",
    "# Đánh giá tính phù hợp của tài liệu được truy xuất\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"Đánh giá sự phù hợp của tài liệu truy xuất\"\"\"\n",
    "    # Logic đánh giá\n",
    "    return True/False\n",
    "\n",
    "# Tiến hành đánh giá toàn diện\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    # Thực hiện RAG và trả về kết quả\n",
    "    docs = retriever.invoke(question)\n",
    "    # Xử lý và trả về kết quả\n",
    "    return {\"answer\": answer, \"documents\": docs}\n",
    "\n",
    "# Đánh giá toàn bộ hệ thống\n",
    "experiment_results = client.evaluate(\n",
    "    rag_bot,\n",
    "    dataset=\"your_dataset\",\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-evaluation\",\n",
    ")\n",
    "```\n",
    "\n",
    "### Tính năng nâng cao:\n",
    "- Đánh giá tính chính xác (correctness)\n",
    "- Đánh giá tính phù hợp (relevance)\n",
    "- Đánh giá tính có cơ sở (groundedness)\n",
    "- Đánh giá retrieval (retrieval metrics)\n",
    "- Tích hợp với công cụ đánh giá ngoài như RAGAS\n",
    "- Visualizing và dashboard để theo dõi hiệu suất\n",
    "\n",
    "## Tổng Hợp Các Repository RAG với LangChain Đáng Chú Ý\n",
    "\n",
    "1. **[langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch)** - Repository chính thức từ LangChain triển khai RAG từ cơ bản đến nâng cao\n",
    "\n",
    "2. **[NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_Techniques)** - Bộ sưu tập toàn diện về các kỹ thuật RAG được phân loại theo các layer:\n",
    "   - Foundational Techniques\n",
    "   - Query Enhancement\n",
    "   - Context Enrichment\n",
    "   - Advanced Retrieval\n",
    "   - Iterative Techniques\n",
    "   - Evaluation\n",
    "   - Advanced Architecture\n",
    "\n",
    "3. **[prathameshks/RAG-using-langchain](https://github.com/prathameshks/RAG-using-langchain)** - Triển khai RAG hoàn chỉnh với các layer cơ bản, sử dụng LangChain và ChromaDB\n",
    "\n",
    "4. **[romilandc/langchain-RAG](https://github.com/romilandc/langchain-RAG)** - Ứng dụng RAG cơ bản sử dụng Chroma vector database\n",
    "\n",
    "5. **[mlsmall/RAG-Application-with-LangChain](https://github.com/mlsmall/RAG-Application-with-LangChain)** - Ứng dụng RAG sử dụng LangChain và OpenAI\n",
    "\n",
    "## Kết Luận\n",
    "\n",
    "Kiến trúc RAG với LangChain được xây dựng từ 6 lớp chính, mỗi lớp đều có các module/components riêng và dễ dàng mở rộng:\n",
    "\n",
    "1. **Data Ingestion Layer**: Tải và tiền xử lý dữ liệu\n",
    "2. **Chunking Layer**: Chia nhỏ tài liệu theo nhiều chiến lược khác nhau\n",
    "3. **Embedding Layer**: Chuyển đổi văn bản thành vector và lưu trữ\n",
    "4. **Retrieval Layer**: Truy xuất thông tin liên quan dựa trên câu hỏi\n",
    "5. **Generation Layer**: Sinh nội dung từ thông tin truy xuất được\n",
    "6. **Evaluation Layer**: Đánh giá chất lượng của toàn bộ hệ thống\n",
    "\n",
    "LangChain cung cấp các framework và thư viện để triển khai tất cả các lớp này một cách linh hoạt, đơn giản và có thể mở rộng. Điều này cho phép xây dựng các hệ thống RAG từ đơn giản đến phức tạp, đáp ứng nhiều nhu cầu khác nhau.\n",
    "\n",
    "\n",
    "==========\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 cách chia khác đi 1 tí "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là các best practices (thực tiễn tốt nhất) để triển khai kiến trúc RAG (Retrieval-Augmented Generation) với LangChain, được tổng hợp từ các nguồn tài liệu và kinh nghiệm thực tế:\n",
    "\n",
    "---\n",
    "\n",
    "#  Offline Preparation (Chuẩn bị Dữ liệu)\n",
    "\n",
    "## 🧩 1.1. Data Ingestion & Chunking\n",
    "\n",
    "- **Làm sạch và chuẩn hóa dữ liệu**: Trước khi xử lý, đảm bảo dữ liệu được làm sạch và chuẩn hóa để cải thiện chất lượng truy xuất và sinh nội dung.\n",
    "\n",
    "- **Chiến lược chunking phù hợp**: Sử dụng các chiến lược chunking như chia theo đoạn văn, câu, hoặc dựa trên ngữ nghĩa để giữ nguyên ngữ cảnh và cải thiện hiệu quả truy xuất.\n",
    "\n",
    "- **Kích thước chunk tối ưu**: Điều chỉnh kích thước chunk để cân bằng giữa độ dài ngữ cảnh và giới hạn token của mô hình ngôn ngữ.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 1.2. Embedding & Vector Store\n",
    "\n",
    "- **Lựa chọn mô hình embedding phù hợp**: Chọn mô hình embedding phù hợp với ngôn ngữ và lĩnh vực cụ thể để cải thiện độ chính xác của truy xuất.\n",
    "\n",
    "- **Tối ưu hóa vector store**: Sử dụng các vector store như FAISS, Chroma, hoặc Pinecone tùy thuộc vào yêu cầu về hiệu suất và khả năng mở rộng.\n",
    "\n",
    "- **Kết hợp tìm kiếm vector và từ khóa**: Kết hợp tìm kiếm dựa trên vector và từ khóa (BM25) để cải thiện độ chính xác và độ bao phủ của truy xuất.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Online Query Processing (Xử lý Truy vấn Trực tuyến)\n",
    "\n",
    "## 🔍 2.1. Retrieval Strategies\n",
    "\n",
    "- **Tùy chỉnh retriever**: Điều chỉnh các tham số của retriever như số lượng kết quả trả về, ngưỡng độ tương đồng, và chiến lược reranking để tối ưu hóa kết quả truy xuất.\n",
    "\n",
    "- **Sử dụng hybrid retriever**: Kết hợp nhiều phương pháp truy xuất như BM25 và vector search để tận dụng ưu điểm của từng phương pháp.\n",
    "\n",
    "- **Áp dụng kỹ thuật reranking**: Sử dụng các mô hình reranking để sắp xếp lại kết quả truy xuất dựa trên độ liên quan đến truy vấn.\n",
    "\n",
    "---\n",
    "\n",
    "## ✍️ 2.2. Prompt Engineering & Generation\n",
    "\n",
    "- **Thiết kế prompt hiệu quả**: Tạo các prompt rõ ràng và cụ thể để hướng dẫn mô hình ngôn ngữ sinh nội dung chính xác và phù hợp.\n",
    "\n",
    "- **Sử dụng template prompt**: Sử dụng các template prompt để đảm bảo tính nhất quán và dễ dàng bảo trì.\n",
    "\n",
    "- **Tối ưu hóa độ dài prompt**: Giữ cho prompt ngắn gọn nhưng đầy đủ thông tin cần thiết để tránh vượt quá giới hạn token của mô hình.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Evaluation and Deployment (Đánh giá và Triển khai)\n",
    "\n",
    "## 📊 3.1. Evaluation & Monitoring\n",
    "\n",
    "- **Đánh giá hiệu suất hệ thống**: Sử dụng các chỉ số như độ chính xác, độ bao phủ, và độ liên quan để đánh giá hiệu suất của hệ thống RAG.\n",
    "\n",
    "- **Theo dõi và ghi log**: Ghi lại các truy vấn, kết quả truy xuất, và phản hồi của người dùng để phân tích và cải thiện hệ thống.\n",
    "\n",
    "- **Thử nghiệm A/B**: Thực hiện các thử nghiệm A/B để so sánh hiệu quả của các cấu hình và chiến lược khác nhau.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 3.2. Deployment & Scalability\n",
    "\n",
    "- **Tối ưu hóa hiệu suất**: Sử dụng các kỹ thuật như caching, batch processing, và parallel processing để cải thiện hiệu suất của hệ thống.\n",
    "\n",
    "- **Đảm bảo khả năng mở rộng**: Thiết kế hệ thống với khả năng mở rộng để xử lý khối lượng dữ liệu và truy vấn lớn.\n",
    "\n",
    "- **Bảo mật và quyền riêng tư**: Đảm bảo dữ liệu được bảo mật và tuân thủ các quy định về quyền riêng tư.\n",
    "\n",
    "---\n",
    "\n",
    "Việc áp dụng các best practices trên sẽ giúp bạn xây dựng một hệ thống RAG hiệu quả, chính xác, và dễ dàng mở rộng. Nếu bạn cần hỗ trợ thêm về triển khai cụ thể hoặc ví dụ mã nguồn, hãy cho tôi biết! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây là bảng tổng hợp các đối tác và công cụ mà LangChain có thể tích hợp trong từng giai đoạn của kiến trúc RAG (Retrieval-Augmented Generation), được chia thành ba nhóm chính: **Chuẩn bị Dữ liệu (Offline Preparation)**, **Xử lý Truy vấn Trực tuyến (Online Query Processing)** và **Đánh giá & Triển khai (Evaluation & Deployment)**. Mỗi nhóm bao gồm hai bước liên tiếp trong quy trình RAG.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 1. Chuẩn bị Dữ liệu (Offline Preparation)\n",
    "\n",
    "### 🧩 1.1. Nạp Dữ liệu & Chia Nhỏ (Data Ingestion & Chunking)\n",
    "\n",
    "- **Công cụ Nạp Dữ liệu**:\n",
    "  - *LangChain Document Loaders*: Hỗ trợ nhiều định dạng như PDF, HTML, CSV, Markdown, Notion, v.v.\n",
    "  - *LlamaIndex*: Cung cấp khả năng phân tích và chia nhỏ tài liệu linh hoạt.\n",
    "  - *Unstructured*: Thư viện xử lý và trích xuất nội dung từ các tài liệu thô.\n",
    "  - *PyPDF, PyMuPDF*: Thư viện Python để đọc và xử lý tệp PDF.\n",
    "\n",
    "- **Chiến lược Chia Nhỏ**:\n",
    "  - *RecursiveCharacterTextSplitter*: Chia nhỏ văn bản dựa trên ký tự với khả năng chồng lấp.\n",
    "  - *SemanticChunker*: Chia nhỏ văn bản dựa trên ngữ nghĩa sử dụng mô hình embedding.\n",
    "\n",
    "### 🧠 1.2. Nhúng & Lưu trữ Vector (Embedding & Vector Store)\n",
    "\n",
    "- **Mô hình Nhúng (Embeddings)**:\n",
    "  - *OpenAI Embeddings*: Mô hình nhúng từ OpenAI, như `text-embedding-ada-002`.\n",
    "  - *HuggingFace Transformers*: Cung cấp nhiều mô hình nhúng như BERT, RoBERTa.\n",
    "  - *SentenceTransformers*: Mô hình nhúng câu hiệu quả cho nhiều ngôn ngữ.\n",
    "\n",
    "- **Cơ sở Dữ liệu Vector (Vector Stores)**:\n",
    "  - *FAISS*: Phù hợp cho các ứng dụng cục bộ hoặc trong bộ nhớ với tìm kiếm tương tự nhanh chóng.\n",
    "  - *ChromaDB*: Cơ sở dữ liệu vector nhẹ và hiệu quả cho lưu trữ vector đơn giản.\n",
    "  - *Pinecone*: Cơ sở dữ liệu vector đám mây với khả năng mở rộng cao.\n",
    "  - *Weaviate*: Hỗ trợ tìm kiếm kết hợp dữ liệu có cấu trúc và không cấu trúc.\n",
    "  - *Qdrant*: Tối ưu cho các ứng dụng AI thời gian thực với tìm kiếm lân cận gần đúng (ANN).\n",
    "  - *MongoDB Atlas Vector Search*: Cung cấp khả năng tìm kiếm vector tích hợp trong MongoDB.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ 2. Xử lý Truy vấn Trực tuyến (Online Query Processing)\n",
    "\n",
    "### 🔍 2.1. Chiến lược Truy Xuất (Retrieval Strategies)\n",
    "\n",
    "- **Retrievers**:\n",
    "  - *BM25Retriever*: Truy xuất dựa trên từ khóa.\n",
    "  - *VectorRetriever*: Truy xuất dựa trên vector embedding.\n",
    "  - *EnsembleRetriever*: Kết hợp nhiều phương pháp truy xuất.\n",
    "  - *ContextualCompressionRetriever*: Nén và lọc kết quả truy xuất để tối ưu hóa độ chính xác.\n",
    "\n",
    "- **Chiến lược Truy Xuất Nâng Cao**:\n",
    "  - *Hybrid Search*: Kết hợp tìm kiếm từ khóa và vector.\n",
    "  - *Self-query Retriever*: Tự động tạo filter từ câu hỏi người dùng.\n",
    "  - *Multi-query Retriever*: Tạo nhiều truy vấn khác nhau từ một câu hỏi để cải thiện độ bao phủ.\n",
    "  - *Rerankers*: Sắp xếp lại các kết quả để tối ưu hóa độ chính xác, có thể sử dụng mô hình cross-encoder.\n",
    "\n",
    "### ✍️ 2.2. Kỹ thuật Prompt & Sinh Nội Dung (Prompt Engineering & Generation)\n",
    "\n",
    "- **Mô hình Ngôn ngữ Lớn (LLMs)**:\n",
    "  - *OpenAI GPT-4, GPT-3.5*: Mô hình ngôn ngữ tiên tiến từ OpenAI.\n",
    "  - *Anthropic Claude*: Mô hình ngôn ngữ từ Anthropic.\n",
    "  - *Cohere, MistralAI, Baichuan*: Các mô hình ngôn ngữ khác hỗ trợ tích hợp với LangChain.\n",
    "\n",
    "- **Prompt Templates**:\n",
    "  - *LangChain Hub*: Cung cấp các mẫu prompt có sẵn để sử dụng.\n",
    "  - *LangGraph*: Hỗ trợ xây dựng luồng xử lý tùy chỉnh với các prompt phức tạp.\n",
    "\n",
    "- **Kỹ thuật Tối Ưu Prompt**:\n",
    "  - *Prompt Chaining*: Kết hợp nhiều prompt để xử lý các tác vụ phức tạp.\n",
    "  - *Prompt Tuning*: Tinh chỉnh prompt để cải thiện hiệu suất mô hình.\n",
    "  - *Few-shot Prompting*: Cung cấp ví dụ trong prompt để hướng dẫn mô hình.\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 3. Đánh giá & Triển khai (Evaluation & Deployment)\n",
    "\n",
    "### 📊 3.1. Đánh Giá & Giám Sát (Evaluation & Monitoring)\n",
    "\n",
    "- **Công cụ Đánh Giá**:\n",
    "  - *LangSmith*: Công cụ theo dõi và đánh giá các ứng dụng RAG.\n",
    "  - *RAGAS*: Framework đánh giá RAG với các chỉ số như độ chính xác, độ phù hợp, độ có cơ sở.\n",
    "\n",
    "- **Chỉ số Đánh Giá**:\n",
    "  - *Độ Chính Xác (Correctness)*: Đánh giá tính đúng đắn của câu trả lời.\n",
    "  - *Độ Phù Hợp (Relevance)*: Đánh giá mức độ liên quan của câu trả lời với câu hỏi.\n",
    "  - *Độ Có Cơ Sở (Groundedness)*: Đánh giá mức độ dựa trên thông tin được truy xuất.\n",
    "  - *Đánh Giá Truy Xuất (Retrieval Metrics)*: Đánh giá hiệu suất của hệ thống truy xuất.\n",
    "\n",
    "### 📦 3.2. Triển Khai & Khả Năng Mở Rộng (Deployment & Scalability)\n",
    "\n",
    "- **Nền Tảng Triển Khai**:\n",
    "  - *FastAPI*: Framework Python hiệu suất cao cho việc xây dựng API.\n",
    "  - *LangChain.js*: Thư viện JavaScript để xây dựng ứng dụng RAG.\n",
    "  - *Azure Static Web Apps, Azure Container Apps*: Dịch vụ triển khai ứng dụng trên Azure.\n",
    "  - *Google Cloud Vertex AI*: Nền tảng AI từ Google hỗ trợ triển khai mô hình ngôn ngữ lớn.\n",
    "\n",
    "- **Chiến lược Tối Ưu Hóa**:\n",
    "  - *Caching*: Lưu trữ kết quả truy vấn để giảm thời gian phản hồi.\n",
    "  - *Batch Processing*: Xử lý hàng loạt để cải thiện hiệu suất.\n",
    "  - *Parallel Processing*: Xử lý song song để tăng tốc độ xử lý.\n",
    "  - *A/B Testing*: Thử nghiệm các cấu hình khác nhau để chọn ra phương án tối ưu.\n",
    "\n",
    "---\n",
    "\n",
    "Việc áp dụng các công cụ và đối tác phù hợp trong từng giai đoạn của kiến trúc RAG sẽ giúp bạn xây dựng một hệ thống hiệu quả, chính xác và dễ dàng mở rộng. Nếu bạn cần hỗ trợ thêm về triển khai cụ thể hoặc ví dụ mã nguồn, hãy cho tôi biết! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
