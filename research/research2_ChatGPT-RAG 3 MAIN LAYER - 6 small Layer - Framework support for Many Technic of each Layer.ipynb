{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pha offline \n",
    "- Chunking -> Embedding -> Save Vector DB. \n",
    "# 2. Pha Online: \n",
    "- Retrieval\n",
    "- Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. Data Ingestion Layer\n",
    "   ‚îú‚îÄ‚îÄ Document Loaders (PDF, HTML, Text, etc.)\n",
    "   ‚îú‚îÄ‚îÄ Text Extraction\n",
    "   ‚îî‚îÄ‚îÄ Data Cleaning\n",
    "\n",
    "2. Chunking Layer\n",
    "   ‚îú‚îÄ‚îÄ Strategy Selector (Size-based, Semantic, Recursive)\n",
    "   ‚îî‚îÄ‚îÄ Chunking Configuration\n",
    "\n",
    "3. Embedding Layer\n",
    "   ‚îú‚îÄ‚îÄ Model Selector (OpenAI, HuggingFace, etc.)\n",
    "   ‚îî‚îÄ‚îÄ Embedding Configuration\n",
    "\n",
    "4. Retrieval Layer\n",
    "   ‚îú‚îÄ‚îÄ Retriever Selector (BM25, Vector, Hybrid)\n",
    "   ‚îú‚îÄ‚îÄ Reranking Integration\n",
    "   ‚îî‚îÄ‚îÄ Retrieval Configuration\n",
    "\n",
    "5. Generation Layer\n",
    "   ‚îú‚îÄ‚îÄ LLM Selector\n",
    "   ‚îú‚îÄ‚îÄ Prompt Templates\n",
    "   ‚îî‚îÄ‚îÄ Response Generation\n",
    "\n",
    "6. Evaluation Layer\n",
    "   ‚îú‚îÄ‚îÄ Metrics (Precision, Recall, Relevance)\n",
    "   ‚îú‚îÄ‚îÄ Benchmarking\n",
    "   ‚îî‚îÄ‚îÄ Performance Tracking\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT l√† file PDF \n",
    "---\n",
    "output l√† lu·ªìng RAG v√† ch·∫°y benchmark ƒë∆∞·ª£c v·ªõi c√°c k·ªπ thu·∫≠t kh√°c nhau trong 6 ph·∫ßn \n",
    "\n",
    "```\n",
    "1. Data Ingestion Layer\n",
    "   ‚îú‚îÄ‚îÄ Document Loaders (PDF, HTML, Text, etc.)\n",
    "   ‚îú‚îÄ‚îÄ Text Extraction\n",
    "   ‚îî‚îÄ‚îÄ Data Cleaning\n",
    "\n",
    "2. Chunking Layer\n",
    "   ‚îú‚îÄ‚îÄ Strategy Selector (Size-based, Semantic, Recursive)\n",
    "   ‚îî‚îÄ‚îÄ Chunking Configuration\n",
    "\n",
    "3. Embedding Layer\n",
    "   ‚îú‚îÄ‚îÄ Model Selector (OpenAI, HuggingFace, etc.)\n",
    "   ‚îî‚îÄ‚îÄ Embedding Configuration\n",
    "\n",
    "4. Retrieval Layer\n",
    "   ‚îú‚îÄ‚îÄ Retriever Selector (BM25, Vector, Hybrid)\n",
    "   ‚îú‚îÄ‚îÄ Reranking Integration\n",
    "   ‚îî‚îÄ‚îÄ Retrieval Configuration\n",
    "\n",
    "5. Generation Layer\n",
    "   ‚îú‚îÄ‚îÄ LLM Selector\n",
    "   ‚îú‚îÄ‚îÄ Prompt Templates\n",
    "   ‚îî‚îÄ‚îÄ Response Generation\n",
    "\n",
    "6. Evaluation Layer\n",
    "   ‚îú‚îÄ‚îÄ Metrics (Precision, Recall, Relevance)\n",
    "   ‚îú‚îÄ‚îÄ Benchmarking\n",
    "   ‚îî‚îÄ‚îÄ Performance Tracking\n",
    "```\n",
    "\n",
    "th√¨ FLASH RAG h·ªó tr·ª£ ko √Ω "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Code: Ban ƒë·∫ßu t·∫°o 6 folder, code a Minh refactor th√†nh 5 folder, xong fix m√£i ko ƒë∆∞·ª£c => Genspark chia ra cho 6 ph·∫ßn. Input n√≥ v√†o cursorx (xo√° h·∫øt code c≈© ƒëi), v√† k·∫øt qu·∫£ qu√° ngon. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# T·ªïng h·ª£p v·ªÅ Ki·∫øn tr√∫c RAG (Retrieval-Augmented Generation) v·ªõi LangChain\n",
    "\n",
    "D·ª±a tr√™n t√¨m hi·ªÉu t·ª´ c√°c ngu·ªìn t√†i li·ªáu v√† m√£ ngu·ªìn c·ªßa c·ªông ƒë·ªìng, d∆∞·ªõi ƒë√¢y l√† chi ti·∫øt v·ªÅ c√°ch tri·ªÉn khai ki·∫øn tr√∫c RAG (Retrieval-Augmented Generation) s·ª≠ d·ª•ng LangChain theo t·ª´ng layer nh∆∞ b·∫°n ƒë√£ ƒë·ªÅ c·∫≠p:\n",
    "\n",
    "## 1. Data Ingestion Layer (L·ªõp N·∫°p D·ªØ Li·ªáu)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám t·∫£i d·ªØ li·ªáu t·ª´ c√°c ngu·ªìn kh√°c nhau v√† chuy·ªÉn ƒë·ªïi th√†nh c√°c ƒë·ªëi t∆∞·ª£ng Document - ƒë·ªãnh d·∫°ng ti√™u chu·∫©n c·ªßa LangChain.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **Document Loaders**: Class ƒë·ªÉ t·∫£i d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau\n",
    "- **Data Cleaning**: Ti·ªÅn x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, CSVLoader\n",
    "import bs4\n",
    "\n",
    "# T·∫£i d·ªØ li·ªáu t·ª´ trang web\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "web_docs = web_loader.load()\n",
    "\n",
    "# T·∫£i d·ªØ li·ªáu t·ª´ PDF\n",
    "pdf_loader = PyPDFLoader(\"path/to/document.pdf\")\n",
    "pdf_docs = pdf_loader.load()\n",
    "\n",
    "# T·∫£i d·ªØ li·ªáu t·ª´ CSV\n",
    "csv_loader = CSVLoader(\"path/to/data.csv\")\n",
    "csv_docs = csv_loader.load()\n",
    "\n",
    "# K·∫øt h·ª£p t·∫•t c·∫£ t√†i li·ªáu\n",
    "all_docs = web_docs + pdf_docs + csv_docs\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- H·ªó tr·ª£ nhi·ªÅu lo·∫°i ngu·ªìn d·ªØ li·ªáu: PDF, HTML, Text, Office docs, Markdown, Notion, v.v.\n",
    "- T√≠ch h·ª£p v·ªõi nhi·ªÅu API v√† c∆° s·ªü d·ªØ li·ªáu\n",
    "- Kh·∫£ nƒÉng x·ª≠ l√Ω nhi·ªÅu ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o v√† chuy·ªÉn ƒë·ªïi th√†nh Document chu·∫©n\n",
    "\n",
    "## 2. Chunking Layer (L·ªõp Chia Nh·ªè)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám ph√¢n chia c√°c t√†i li·ªáu d√†i th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n, d·ªÖ d√†ng h∆°n cho vi·ªác embedding v√† retrieval.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **Text Splitters**: Class ƒë·ªÉ chia nh·ªè t√†i li·ªáu v·ªõi nhi·ªÅu chi·∫øn l∆∞·ª£c kh√°c nhau\n",
    "- **Chunking Strategies**: C√°c chi·∫øn l∆∞·ª£c chia nh·ªè nh∆∞ size-based, semantic, recursive\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Ph∆∞∆°ng ph√°p chia nh·ªè d·ª±a tr√™n ƒë·ªô d√†i chu·ªói k√Ω t·ª±\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # S·ªë k√Ω t·ª± t·ªëi ƒëa cho m·ªói chunk\n",
    "    chunk_overlap=200,  # S·ªë k√Ω t·ª± ch·ªìng l·∫•p gi·ªØa c√°c chunk\n",
    "    add_start_index=True,  # Theo d√µi v·ªã tr√≠ b·∫Øt ƒë·∫ßu trong t√†i li·ªáu g·ªëc\n",
    ")\n",
    "text_chunks = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Ph∆∞∆°ng ph√°p chia nh·ªè d·ª±a tr√™n ng·ªØ nghƒ©a\n",
    "semantic_splitter = SemanticChunker(\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    breakpoint_threshold_type=\"percentile\"\n",
    ")\n",
    "semantic_chunks = semantic_splitter.split_documents(all_docs)\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- Chia nh·ªè d·ª±a tr√™n k√≠ch th∆∞·ªõc (size-based chunking)\n",
    "- Chia nh·ªè theo ng·ªØ nghƒ©a (semantic chunking)\n",
    "- Chia nh·ªè ƒë·ªá quy (recursive chunking)\n",
    "- Chia nh·ªè theo ƒëo·∫°n vƒÉn ho·∫∑c c·∫•u tr√∫c t√†i li·ªáu\n",
    "- Proposition chunking (t√°ch th√†nh c√°c m·ªánh ƒë·ªÅ ho·∫∑c c√¢u c√≥ nghƒ©a)\n",
    "\n",
    "## 3. Embedding Layer (L·ªõp Nh√∫ng)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám chuy·ªÉn ƒë·ªïi c√°c ƒëo·∫°n vƒÉn b·∫£n th√†nh vector s·ªë (embeddings) ƒë·ªÉ h·ªó tr·ª£ t√¨m ki·∫øm ng·ªØ nghƒ©a.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **Embedding Models**: M√¥ h√¨nh ƒë·ªÉ chuy·ªÉn ƒë·ªïi text th√†nh vector\n",
    "- **Vector Stores**: C∆° s·ªü d·ªØ li·ªáu l∆∞u tr·ªØ vector\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Kh·ªüi t·∫°o embedding model\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# T·∫°o vector store trong b·ªô nh·ªõ\n",
    "memory_vector_store = InMemoryVectorStore(embeddings)\n",
    "document_ids = memory_vector_store.add_documents(documents=text_chunks)\n",
    "\n",
    "# Ho·∫∑c s·ª≠ d·ª•ng Chroma DB ƒë·ªÉ l∆∞u tr·ªØ\n",
    "db = Chroma.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "# Ho·∫∑c s·ª≠ d·ª•ng FAISS (Facebook AI Similarity Search)\n",
    "faiss_db = FAISS.from_documents(\n",
    "    documents=text_chunks,\n",
    "    embedding=embeddings\n",
    ")\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- H·ªó tr·ª£ nhi·ªÅu m√¥ h√¨nh embedding: OpenAI, HuggingFace, SentenceTransformers, v.v.\n",
    "- T√≠ch h·ª£p v·ªõi nhi·ªÅu vector database: Chroma, FAISS, Pinecone, Weaviate, Milvus, v.v.\n",
    "- C·∫•u h√¨nh v√† t·ªëi ∆∞u h√≥a c√°c tham s·ªë embedding\n",
    "\n",
    "## 4. Retrieval Layer (L·ªõp Truy Xu·∫•t)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám truy xu·∫•t c√°c ƒëo·∫°n vƒÉn b·∫£n li√™n quan nh·∫•t t·ª´ vector store d·ª±a tr√™n c√°c truy v·∫•n ƒë·∫ßu v√†o.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **Retrievers**: Class truy xu·∫•t t√†i li·ªáu t·ª´ vector stores\n",
    "- **Retrieval Strategies**: C√°c chi·∫øn l∆∞·ª£c truy xu·∫•t nh∆∞ BM25, Vector Search, Hybrid Search\n",
    "- **Rerankers**: Class s·∫Øp x·∫øp l·∫°i c√°c k·∫øt qu·∫£ ƒë·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªô ch√≠nh x√°c\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_community.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain_community.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_community.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# Vector retriever c∆° b·∫£n\n",
    "vector_retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# BM25 Retriever (t·ª´ kh√≥a)\n",
    "bm25_retriever = BM25Retriever.from_documents(text_chunks)\n",
    "bm25_retriever.k = 4\n",
    "\n",
    "# Ensemble Retriever (k·∫øt h·ª£p nhi·ªÅu lo·∫°i)\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever], \n",
    "    weights=[0.7, 0.3]\n",
    ")\n",
    "\n",
    "# Contextual Compression Retriever (n√©n v√† l·ªçc k·∫øt qu·∫£)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_retriever\n",
    ")\n",
    "\n",
    "# S·ª≠ d·ª•ng retriever ƒë·ªÉ l·∫•y t√†i li·ªáu li√™n quan\n",
    "query = \"What is Task Decomposition?\"\n",
    "docs = vector_retriever.invoke(query)\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- Truy xu·∫•t ng·ªØ nghƒ©a (Semantic Retrieval)\n",
    "- Reranking k·∫øt qu·∫£ b·∫±ng m√¥ h√¨nh cross-encoder\n",
    "- Hybrid Search (k·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p t√¨m ki·∫øm)\n",
    "- Self-query (t·ª± ƒë·ªông t·∫°o filter t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng)\n",
    "- Multi-query retrieval (t·∫°o nhi·ªÅu truy v·∫•n kh√°c nhau t·ª´ m·ªôt c√¢u h·ªèi)\n",
    "- Query transformation (bi·∫øn ƒë·ªïi c√¢u h·ªèi ƒë·ªÉ t·ªëi ∆∞u k·∫øt qu·∫£ t√¨m ki·∫øm)\n",
    "\n",
    "## 5. Generation Layer (L·ªõp Sinh N·ªôi Dung)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám sinh n·ªôi dung d·ª±a tr√™n th√¥ng tin truy xu·∫•t ƒë∆∞·ª£c v√† c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **LLMs/ChatModels**: M√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ sinh n·ªôi dung\n",
    "- **Prompt Templates**: M·∫´u l·ªùi nh·∫Øc ƒë·ªÉ h∆∞·ªõng d·∫´n m√¥ h√¨nh sinh n·ªôi dung\n",
    "- **Chains/Graphs**: Lu·ªìng x·ª≠ l√Ω ƒë·ªÉ k·∫øt h·ª£p truy xu·∫•t v√† sinh n·ªôi dung\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh ng√¥n ng·ªØ\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# S·ª≠ d·ª•ng prompt c√≥ s·∫µn t·ª´ hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a state cho ·ª©ng d·ª•ng\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a c√°c b∆∞·ªõc x·ª≠ l√Ω\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# T·∫°o graph v√† compile\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"retrieve\", retrieve)\n",
    "graph_builder.add_node(\"generate\", generate)\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# S·ª≠ d·ª•ng graph ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi\n",
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- T√πy ch·ªânh prompt templates theo nhi·ªÅu chi·∫øn l∆∞·ª£c kh√°c nhau\n",
    "- Contextual generation (sinh n·ªôi dung d·ª±a tr√™n ng·ªØ c·∫£nh)\n",
    "- Iterative refinement (tinh ch·ªânh n·ªôi dung qua nhi·ªÅu l·∫ßn l·∫∑p)\n",
    "- B·ªï sung meta-information v√†o c√¢u tr·∫£ l·ªùi\n",
    "- T√≠ch h·ª£p v·ªõi nhi·ªÅu m√¥ h√¨nh ng√¥n ng·ªØ kh√°c nhau: OpenAI, Anthropic, Google, v.v.\n",
    "\n",
    "## 6. Evaluation Layer (L·ªõp ƒê√°nh Gi√°)\n",
    "\n",
    "L·ªõp n√†y ch·ªãu tr√°ch nhi·ªám ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa h·ªá th·ªëng RAG, t·ª´ retrieval ƒë·∫øn generation.\n",
    "\n",
    "### C√°c th√†nh ph·∫ßn ch√≠nh:\n",
    "- **Evaluators**: ƒê√°nh gi√° c√°c kh√≠a c·∫°nh kh√°c nhau c·ªßa h·ªá th·ªëng\n",
    "- **Metrics**: C√°c th∆∞·ªõc ƒëo ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t\n",
    "- **Benchmarking**: So s√°nh hi·ªáu su·∫•t gi·ªØa c√°c c·∫•u h√¨nh kh√°c nhau\n",
    "- **LangSmith**: C√¥ng c·ª• theo d√µi v√† ƒë√°nh gi√° c√°c ·ª©ng d·ª•ng RAG\n",
    "\n",
    "### V√≠ d·ª• code tri·ªÉn khai:\n",
    "\n",
    "```python\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "\n",
    "# Kh·ªüi t·∫°o client LangSmith\n",
    "client = Client()\n",
    "\n",
    "# T·∫°o h√†m ƒë√°nh gi√° t√≠nh ch√≠nh x√°c\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"ƒê√°nh gi√° t√≠nh ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "    # Logic ƒë√°nh gi√°\n",
    "    return True/False\n",
    "\n",
    "# ƒê√°nh gi√° s·ª± ph√π h·ª£p c·ªßa c√¢u tr·∫£ l·ªùi v·ªõi c√¢u h·ªèi\n",
    "def relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"ƒê√°nh gi√° s·ª± ph√π h·ª£p c·ªßa c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "    # Logic ƒë√°nh gi√°\n",
    "    return True/False\n",
    "\n",
    "# ƒê√°nh gi√° t√≠nh grounded (d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c truy xu·∫•t)\n",
    "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"ƒê√°nh gi√° t√≠nh grounded c·ªßa c√¢u tr·∫£ l·ªùi\"\"\"\n",
    "    # Logic ƒë√°nh gi√°\n",
    "    return True/False\n",
    "\n",
    "# ƒê√°nh gi√° t√≠nh ph√π h·ª£p c·ªßa t√†i li·ªáu ƒë∆∞·ª£c truy xu·∫•t\n",
    "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
    "    \"\"\"ƒê√°nh gi√° s·ª± ph√π h·ª£p c·ªßa t√†i li·ªáu truy xu·∫•t\"\"\"\n",
    "    # Logic ƒë√°nh gi√°\n",
    "    return True/False\n",
    "\n",
    "# Ti·∫øn h√†nh ƒë√°nh gi√° to√†n di·ªán\n",
    "@traceable()\n",
    "def rag_bot(question: str) -> dict:\n",
    "    # Th·ª±c hi·ªán RAG v√† tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    docs = retriever.invoke(question)\n",
    "    # X·ª≠ l√Ω v√† tr·∫£ v·ªÅ k·∫øt qu·∫£\n",
    "    return {\"answer\": answer, \"documents\": docs}\n",
    "\n",
    "# ƒê√°nh gi√° to√†n b·ªô h·ªá th·ªëng\n",
    "experiment_results = client.evaluate(\n",
    "    rag_bot,\n",
    "    dataset=\"your_dataset\",\n",
    "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
    "    experiment_prefix=\"rag-evaluation\",\n",
    ")\n",
    "```\n",
    "\n",
    "### T√≠nh nƒÉng n√¢ng cao:\n",
    "- ƒê√°nh gi√° t√≠nh ch√≠nh x√°c (correctness)\n",
    "- ƒê√°nh gi√° t√≠nh ph√π h·ª£p (relevance)\n",
    "- ƒê√°nh gi√° t√≠nh c√≥ c∆° s·ªü (groundedness)\n",
    "- ƒê√°nh gi√° retrieval (retrieval metrics)\n",
    "- T√≠ch h·ª£p v·ªõi c√¥ng c·ª• ƒë√°nh gi√° ngo√†i nh∆∞ RAGAS\n",
    "- Visualizing v√† dashboard ƒë·ªÉ theo d√µi hi·ªáu su·∫•t\n",
    "\n",
    "## T·ªïng H·ª£p C√°c Repository RAG v·ªõi LangChain ƒê√°ng Ch√∫ √ù\n",
    "\n",
    "1. **[langchain-ai/rag-from-scratch](https://github.com/langchain-ai/rag-from-scratch)** - Repository ch√≠nh th·ª©c t·ª´ LangChain tri·ªÉn khai RAG t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao\n",
    "\n",
    "2. **[NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_Techniques)** - B·ªô s∆∞u t·∫≠p to√†n di·ªán v·ªÅ c√°c k·ªπ thu·∫≠t RAG ƒë∆∞·ª£c ph√¢n lo·∫°i theo c√°c layer:\n",
    "   - Foundational Techniques\n",
    "   - Query Enhancement\n",
    "   - Context Enrichment\n",
    "   - Advanced Retrieval\n",
    "   - Iterative Techniques\n",
    "   - Evaluation\n",
    "   - Advanced Architecture\n",
    "\n",
    "3. **[prathameshks/RAG-using-langchain](https://github.com/prathameshks/RAG-using-langchain)** - Tri·ªÉn khai RAG ho√†n ch·ªânh v·ªõi c√°c layer c∆° b·∫£n, s·ª≠ d·ª•ng LangChain v√† ChromaDB\n",
    "\n",
    "4. **[romilandc/langchain-RAG](https://github.com/romilandc/langchain-RAG)** - ·ª®ng d·ª•ng RAG c∆° b·∫£n s·ª≠ d·ª•ng Chroma vector database\n",
    "\n",
    "5. **[mlsmall/RAG-Application-with-LangChain](https://github.com/mlsmall/RAG-Application-with-LangChain)** - ·ª®ng d·ª•ng RAG s·ª≠ d·ª•ng LangChain v√† OpenAI\n",
    "\n",
    "## K·∫øt Lu·∫≠n\n",
    "\n",
    "Ki·∫øn tr√∫c RAG v·ªõi LangChain ƒë∆∞·ª£c x√¢y d·ª±ng t·ª´ 6 l·ªõp ch√≠nh, m·ªói l·ªõp ƒë·ªÅu c√≥ c√°c module/components ri√™ng v√† d·ªÖ d√†ng m·ªü r·ªông:\n",
    "\n",
    "1. **Data Ingestion Layer**: T·∫£i v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "2. **Chunking Layer**: Chia nh·ªè t√†i li·ªáu theo nhi·ªÅu chi·∫øn l∆∞·ª£c kh√°c nhau\n",
    "3. **Embedding Layer**: Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector v√† l∆∞u tr·ªØ\n",
    "4. **Retrieval Layer**: Truy xu·∫•t th√¥ng tin li√™n quan d·ª±a tr√™n c√¢u h·ªèi\n",
    "5. **Generation Layer**: Sinh n·ªôi dung t·ª´ th√¥ng tin truy xu·∫•t ƒë∆∞·ª£c\n",
    "6. **Evaluation Layer**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng c·ªßa to√†n b·ªô h·ªá th·ªëng\n",
    "\n",
    "LangChain cung c·∫•p c√°c framework v√† th∆∞ vi·ªán ƒë·ªÉ tri·ªÉn khai t·∫•t c·∫£ c√°c l·ªõp n√†y m·ªôt c√°ch linh ho·∫°t, ƒë∆°n gi·∫£n v√† c√≥ th·ªÉ m·ªü r·ªông. ƒêi·ªÅu n√†y cho ph√©p x√¢y d·ª±ng c√°c h·ªá th·ªëng RAG t·ª´ ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p, ƒë√°p ·ª©ng nhi·ªÅu nhu c·∫ßu kh√°c nhau.\n",
    "\n",
    "\n",
    "==========\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 c√°ch chia kh√°c ƒëi 1 t√≠ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† c√°c best practices (th·ª±c ti·ªÖn t·ªët nh·∫•t) ƒë·ªÉ tri·ªÉn khai ki·∫øn tr√∫c RAG (Retrieval-Augmented Generation) v·ªõi LangChain, ƒë∆∞·ª£c t·ªïng h·ª£p t·ª´ c√°c ngu·ªìn t√†i li·ªáu v√† kinh nghi·ªám th·ª±c t·∫ø:\n",
    "\n",
    "---\n",
    "\n",
    "#  Offline Preparation (Chu·∫©n b·ªã D·ªØ li·ªáu)\n",
    "\n",
    "## üß© 1.1. Data Ingestion & Chunking\n",
    "\n",
    "- **L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu**: Tr∆∞·ªõc khi x·ª≠ l√Ω, ƒë·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng truy xu·∫•t v√† sinh n·ªôi dung.\n",
    "\n",
    "- **Chi·∫øn l∆∞·ª£c chunking ph√π h·ª£p**: S·ª≠ d·ª•ng c√°c chi·∫øn l∆∞·ª£c chunking nh∆∞ chia theo ƒëo·∫°n vƒÉn, c√¢u, ho·∫∑c d·ª±a tr√™n ng·ªØ nghƒ©a ƒë·ªÉ gi·ªØ nguy√™n ng·ªØ c·∫£nh v√† c·∫£i thi·ªán hi·ªáu qu·∫£ truy xu·∫•t.\n",
    "\n",
    "- **K√≠ch th∆∞·ªõc chunk t·ªëi ∆∞u**: ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk ƒë·ªÉ c√¢n b·∫±ng gi·ªØa ƒë·ªô d√†i ng·ªØ c·∫£nh v√† gi·ªõi h·∫°n token c·ªßa m√¥ h√¨nh ng√¥n ng·ªØ.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 1.2. Embedding & Vector Store\n",
    "\n",
    "- **L·ª±a ch·ªçn m√¥ h√¨nh embedding ph√π h·ª£p**: Ch·ªçn m√¥ h√¨nh embedding ph√π h·ª£p v·ªõi ng√¥n ng·ªØ v√† lƒ©nh v·ª±c c·ª• th·ªÉ ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c c·ªßa truy xu·∫•t.\n",
    "\n",
    "- **T·ªëi ∆∞u h√≥a vector store**: S·ª≠ d·ª•ng c√°c vector store nh∆∞ FAISS, Chroma, ho·∫∑c Pinecone t√πy thu·ªôc v√†o y√™u c·∫ßu v·ªÅ hi·ªáu su·∫•t v√† kh·∫£ nƒÉng m·ªü r·ªông.\n",
    "\n",
    "- **K·∫øt h·ª£p t√¨m ki·∫øm vector v√† t·ª´ kh√≥a**: K·∫øt h·ª£p t√¨m ki·∫øm d·ª±a tr√™n vector v√† t·ª´ kh√≥a (BM25) ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c v√† ƒë·ªô bao ph·ªß c·ªßa truy xu·∫•t.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. Online Query Processing (X·ª≠ l√Ω Truy v·∫•n Tr·ª±c tuy·∫øn)\n",
    "\n",
    "## üîç 2.1. Retrieval Strategies\n",
    "\n",
    "- **T√πy ch·ªânh retriever**: ƒêi·ªÅu ch·ªânh c√°c tham s·ªë c·ªßa retriever nh∆∞ s·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ, ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng, v√† chi·∫øn l∆∞·ª£c reranking ƒë·ªÉ t·ªëi ∆∞u h√≥a k·∫øt qu·∫£ truy xu·∫•t.\n",
    "\n",
    "- **S·ª≠ d·ª•ng hybrid retriever**: K·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p truy xu·∫•t nh∆∞ BM25 v√† vector search ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa t·ª´ng ph∆∞∆°ng ph√°p.\n",
    "\n",
    "- **√Åp d·ª•ng k·ªπ thu·∫≠t reranking**: S·ª≠ d·ª•ng c√°c m√¥ h√¨nh reranking ƒë·ªÉ s·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ truy xu·∫•t d·ª±a tr√™n ƒë·ªô li√™n quan ƒë·∫øn truy v·∫•n.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úçÔ∏è 2.2. Prompt Engineering & Generation\n",
    "\n",
    "- **Thi·∫øt k·∫ø prompt hi·ªáu qu·∫£**: T·∫°o c√°c prompt r√µ r√†ng v√† c·ª• th·ªÉ ƒë·ªÉ h∆∞·ªõng d·∫´n m√¥ h√¨nh ng√¥n ng·ªØ sinh n·ªôi dung ch√≠nh x√°c v√† ph√π h·ª£p.\n",
    "\n",
    "- **S·ª≠ d·ª•ng template prompt**: S·ª≠ d·ª•ng c√°c template prompt ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n v√† d·ªÖ d√†ng b·∫£o tr√¨.\n",
    "\n",
    "- **T·ªëi ∆∞u h√≥a ƒë·ªô d√†i prompt**: Gi·ªØ cho prompt ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n token c·ªßa m√¥ h√¨nh.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Evaluation and Deployment (ƒê√°nh gi√° v√† Tri·ªÉn khai)\n",
    "\n",
    "## üìä 3.1. Evaluation & Monitoring\n",
    "\n",
    "- **ƒê√°nh gi√° hi·ªáu su·∫•t h·ªá th·ªëng**: S·ª≠ d·ª•ng c√°c ch·ªâ s·ªë nh∆∞ ƒë·ªô ch√≠nh x√°c, ƒë·ªô bao ph·ªß, v√† ƒë·ªô li√™n quan ƒë·ªÉ ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng RAG.\n",
    "\n",
    "- **Theo d√µi v√† ghi log**: Ghi l·∫°i c√°c truy v·∫•n, k·∫øt qu·∫£ truy xu·∫•t, v√† ph·∫£n h·ªìi c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ ph√¢n t√≠ch v√† c·∫£i thi·ªán h·ªá th·ªëng.\n",
    "\n",
    "- **Th·ª≠ nghi·ªám A/B**: Th·ª±c hi·ªán c√°c th·ª≠ nghi·ªám A/B ƒë·ªÉ so s√°nh hi·ªáu qu·∫£ c·ªßa c√°c c·∫•u h√¨nh v√† chi·∫øn l∆∞·ª£c kh√°c nhau.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 3.2. Deployment & Scalability\n",
    "\n",
    "- **T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t**: S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t nh∆∞ caching, batch processing, v√† parallel processing ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng.\n",
    "\n",
    "- **ƒê·∫£m b·∫£o kh·∫£ nƒÉng m·ªü r·ªông**: Thi·∫øt k·∫ø h·ªá th·ªëng v·ªõi kh·∫£ nƒÉng m·ªü r·ªông ƒë·ªÉ x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu v√† truy v·∫•n l·ªõn.\n",
    "\n",
    "- **B·∫£o m·∫≠t v√† quy·ªÅn ri√™ng t∆∞**: ƒê·∫£m b·∫£o d·ªØ li·ªáu ƒë∆∞·ª£c b·∫£o m·∫≠t v√† tu√¢n th·ªß c√°c quy ƒë·ªãnh v·ªÅ quy·ªÅn ri√™ng t∆∞.\n",
    "\n",
    "---\n",
    "\n",
    "Vi·ªác √°p d·ª•ng c√°c best practices tr√™n s·∫Ω gi√∫p b·∫°n x√¢y d·ª±ng m·ªôt h·ªá th·ªëng RAG hi·ªáu qu·∫£, ch√≠nh x√°c, v√† d·ªÖ d√†ng m·ªü r·ªông. N·∫øu b·∫°n c·∫ßn h·ªó tr·ª£ th√™m v·ªÅ tri·ªÉn khai c·ª• th·ªÉ ho·∫∑c v√≠ d·ª• m√£ ngu·ªìn, h√£y cho t√¥i bi·∫øt! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† b·∫£ng t·ªïng h·ª£p c√°c ƒë·ªëi t√°c v√† c√¥ng c·ª• m√† LangChain c√≥ th·ªÉ t√≠ch h·ª£p trong t·ª´ng giai ƒëo·∫°n c·ªßa ki·∫øn tr√∫c RAG (Retrieval-Augmented Generation), ƒë∆∞·ª£c chia th√†nh ba nh√≥m ch√≠nh: **Chu·∫©n b·ªã D·ªØ li·ªáu (Offline Preparation)**, **X·ª≠ l√Ω Truy v·∫•n Tr·ª±c tuy·∫øn (Online Query Processing)** v√† **ƒê√°nh gi√° & Tri·ªÉn khai (Evaluation & Deployment)**. M·ªói nh√≥m bao g·ªìm hai b∆∞·ªõc li√™n ti·∫øp trong quy tr√¨nh RAG.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ 1. Chu·∫©n b·ªã D·ªØ li·ªáu (Offline Preparation)\n",
    "\n",
    "### üß© 1.1. N·∫°p D·ªØ li·ªáu & Chia Nh·ªè (Data Ingestion & Chunking)\n",
    "\n",
    "- **C√¥ng c·ª• N·∫°p D·ªØ li·ªáu**:\n",
    "  - *LangChain Document Loaders*: H·ªó tr·ª£ nhi·ªÅu ƒë·ªãnh d·∫°ng nh∆∞ PDF, HTML, CSV, Markdown, Notion, v.v.\n",
    "  - *LlamaIndex*: Cung c·∫•p kh·∫£ nƒÉng ph√¢n t√≠ch v√† chia nh·ªè t√†i li·ªáu linh ho·∫°t.\n",
    "  - *Unstructured*: Th∆∞ vi·ªán x·ª≠ l√Ω v√† tr√≠ch xu·∫•t n·ªôi dung t·ª´ c√°c t√†i li·ªáu th√¥.\n",
    "  - *PyPDF, PyMuPDF*: Th∆∞ vi·ªán Python ƒë·ªÉ ƒë·ªçc v√† x·ª≠ l√Ω t·ªáp PDF.\n",
    "\n",
    "- **Chi·∫øn l∆∞·ª£c Chia Nh·ªè**:\n",
    "  - *RecursiveCharacterTextSplitter*: Chia nh·ªè vƒÉn b·∫£n d·ª±a tr√™n k√Ω t·ª± v·ªõi kh·∫£ nƒÉng ch·ªìng l·∫•p.\n",
    "  - *SemanticChunker*: Chia nh·ªè vƒÉn b·∫£n d·ª±a tr√™n ng·ªØ nghƒ©a s·ª≠ d·ª•ng m√¥ h√¨nh embedding.\n",
    "\n",
    "### üß† 1.2. Nh√∫ng & L∆∞u tr·ªØ Vector (Embedding & Vector Store)\n",
    "\n",
    "- **M√¥ h√¨nh Nh√∫ng (Embeddings)**:\n",
    "  - *OpenAI Embeddings*: M√¥ h√¨nh nh√∫ng t·ª´ OpenAI, nh∆∞ `text-embedding-ada-002`.\n",
    "  - *HuggingFace Transformers*: Cung c·∫•p nhi·ªÅu m√¥ h√¨nh nh√∫ng nh∆∞ BERT, RoBERTa.\n",
    "  - *SentenceTransformers*: M√¥ h√¨nh nh√∫ng c√¢u hi·ªáu qu·∫£ cho nhi·ªÅu ng√¥n ng·ªØ.\n",
    "\n",
    "- **C∆° s·ªü D·ªØ li·ªáu Vector (Vector Stores)**:\n",
    "  - *FAISS*: Ph√π h·ª£p cho c√°c ·ª©ng d·ª•ng c·ª•c b·ªô ho·∫∑c trong b·ªô nh·ªõ v·ªõi t√¨m ki·∫øm t∆∞∆°ng t·ª± nhanh ch√≥ng.\n",
    "  - *ChromaDB*: C∆° s·ªü d·ªØ li·ªáu vector nh·∫π v√† hi·ªáu qu·∫£ cho l∆∞u tr·ªØ vector ƒë∆°n gi·∫£n.\n",
    "  - *Pinecone*: C∆° s·ªü d·ªØ li·ªáu vector ƒë√°m m√¢y v·ªõi kh·∫£ nƒÉng m·ªü r·ªông cao.\n",
    "  - *Weaviate*: H·ªó tr·ª£ t√¨m ki·∫øm k·∫øt h·ª£p d·ªØ li·ªáu c√≥ c·∫•u tr√∫c v√† kh√¥ng c·∫•u tr√∫c.\n",
    "  - *Qdrant*: T·ªëi ∆∞u cho c√°c ·ª©ng d·ª•ng AI th·ªùi gian th·ª±c v·ªõi t√¨m ki·∫øm l√¢n c·∫≠n g·∫ßn ƒë√∫ng (ANN).\n",
    "  - *MongoDB Atlas Vector Search*: Cung c·∫•p kh·∫£ nƒÉng t√¨m ki·∫øm vector t√≠ch h·ª£p trong MongoDB.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° 2. X·ª≠ l√Ω Truy v·∫•n Tr·ª±c tuy·∫øn (Online Query Processing)\n",
    "\n",
    "### üîç 2.1. Chi·∫øn l∆∞·ª£c Truy Xu·∫•t (Retrieval Strategies)\n",
    "\n",
    "- **Retrievers**:\n",
    "  - *BM25Retriever*: Truy xu·∫•t d·ª±a tr√™n t·ª´ kh√≥a.\n",
    "  - *VectorRetriever*: Truy xu·∫•t d·ª±a tr√™n vector embedding.\n",
    "  - *EnsembleRetriever*: K·∫øt h·ª£p nhi·ªÅu ph∆∞∆°ng ph√°p truy xu·∫•t.\n",
    "  - *ContextualCompressionRetriever*: N√©n v√† l·ªçc k·∫øt qu·∫£ truy xu·∫•t ƒë·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªô ch√≠nh x√°c.\n",
    "\n",
    "- **Chi·∫øn l∆∞·ª£c Truy Xu·∫•t N√¢ng Cao**:\n",
    "  - *Hybrid Search*: K·∫øt h·ª£p t√¨m ki·∫øm t·ª´ kh√≥a v√† vector.\n",
    "  - *Self-query Retriever*: T·ª± ƒë·ªông t·∫°o filter t·ª´ c√¢u h·ªèi ng∆∞·ªùi d√πng.\n",
    "  - *Multi-query Retriever*: T·∫°o nhi·ªÅu truy v·∫•n kh√°c nhau t·ª´ m·ªôt c√¢u h·ªèi ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô bao ph·ªß.\n",
    "  - *Rerankers*: S·∫Øp x·∫øp l·∫°i c√°c k·∫øt qu·∫£ ƒë·ªÉ t·ªëi ∆∞u h√≥a ƒë·ªô ch√≠nh x√°c, c√≥ th·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh cross-encoder.\n",
    "\n",
    "### ‚úçÔ∏è 2.2. K·ªπ thu·∫≠t Prompt & Sinh N·ªôi Dung (Prompt Engineering & Generation)\n",
    "\n",
    "- **M√¥ h√¨nh Ng√¥n ng·ªØ L·ªõn (LLMs)**:\n",
    "  - *OpenAI GPT-4, GPT-3.5*: M√¥ h√¨nh ng√¥n ng·ªØ ti√™n ti·∫øn t·ª´ OpenAI.\n",
    "  - *Anthropic Claude*: M√¥ h√¨nh ng√¥n ng·ªØ t·ª´ Anthropic.\n",
    "  - *Cohere, MistralAI, Baichuan*: C√°c m√¥ h√¨nh ng√¥n ng·ªØ kh√°c h·ªó tr·ª£ t√≠ch h·ª£p v·ªõi LangChain.\n",
    "\n",
    "- **Prompt Templates**:\n",
    "  - *LangChain Hub*: Cung c·∫•p c√°c m·∫´u prompt c√≥ s·∫µn ƒë·ªÉ s·ª≠ d·ª•ng.\n",
    "  - *LangGraph*: H·ªó tr·ª£ x√¢y d·ª±ng lu·ªìng x·ª≠ l√Ω t√πy ch·ªânh v·ªõi c√°c prompt ph·ª©c t·∫°p.\n",
    "\n",
    "- **K·ªπ thu·∫≠t T·ªëi ∆Øu Prompt**:\n",
    "  - *Prompt Chaining*: K·∫øt h·ª£p nhi·ªÅu prompt ƒë·ªÉ x·ª≠ l√Ω c√°c t√°c v·ª• ph·ª©c t·∫°p.\n",
    "  - *Prompt Tuning*: Tinh ch·ªânh prompt ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh.\n",
    "  - *Few-shot Prompting*: Cung c·∫•p v√≠ d·ª• trong prompt ƒë·ªÉ h∆∞·ªõng d·∫´n m√¥ h√¨nh.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ 3. ƒê√°nh gi√° & Tri·ªÉn khai (Evaluation & Deployment)\n",
    "\n",
    "### üìä 3.1. ƒê√°nh Gi√° & Gi√°m S√°t (Evaluation & Monitoring)\n",
    "\n",
    "- **C√¥ng c·ª• ƒê√°nh Gi√°**:\n",
    "  - *LangSmith*: C√¥ng c·ª• theo d√µi v√† ƒë√°nh gi√° c√°c ·ª©ng d·ª•ng RAG.\n",
    "  - *RAGAS*: Framework ƒë√°nh gi√° RAG v·ªõi c√°c ch·ªâ s·ªë nh∆∞ ƒë·ªô ch√≠nh x√°c, ƒë·ªô ph√π h·ª£p, ƒë·ªô c√≥ c∆° s·ªü.\n",
    "\n",
    "- **Ch·ªâ s·ªë ƒê√°nh Gi√°**:\n",
    "  - *ƒê·ªô Ch√≠nh X√°c (Correctness)*: ƒê√°nh gi√° t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa c√¢u tr·∫£ l·ªùi.\n",
    "  - *ƒê·ªô Ph√π H·ª£p (Relevance)*: ƒê√°nh gi√° m·ª©c ƒë·ªô li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi v·ªõi c√¢u h·ªèi.\n",
    "  - *ƒê·ªô C√≥ C∆° S·ªü (Groundedness)*: ƒê√°nh gi√° m·ª©c ƒë·ªô d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c truy xu·∫•t.\n",
    "  - *ƒê√°nh Gi√° Truy Xu·∫•t (Retrieval Metrics)*: ƒê√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng truy xu·∫•t.\n",
    "\n",
    "### üì¶ 3.2. Tri·ªÉn Khai & Kh·∫£ NƒÉng M·ªü R·ªông (Deployment & Scalability)\n",
    "\n",
    "- **N·ªÅn T·∫£ng Tri·ªÉn Khai**:\n",
    "  - *FastAPI*: Framework Python hi·ªáu su·∫•t cao cho vi·ªác x√¢y d·ª±ng API.\n",
    "  - *LangChain.js*: Th∆∞ vi·ªán JavaScript ƒë·ªÉ x√¢y d·ª±ng ·ª©ng d·ª•ng RAG.\n",
    "  - *Azure Static Web Apps, Azure Container Apps*: D·ªãch v·ª• tri·ªÉn khai ·ª©ng d·ª•ng tr√™n Azure.\n",
    "  - *Google Cloud Vertex AI*: N·ªÅn t·∫£ng AI t·ª´ Google h·ªó tr·ª£ tri·ªÉn khai m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn.\n",
    "\n",
    "- **Chi·∫øn l∆∞·ª£c T·ªëi ∆Øu H√≥a**:\n",
    "  - *Caching*: L∆∞u tr·ªØ k·∫øt qu·∫£ truy v·∫•n ƒë·ªÉ gi·∫£m th·ªùi gian ph·∫£n h·ªìi.\n",
    "  - *Batch Processing*: X·ª≠ l√Ω h√†ng lo·∫°t ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t.\n",
    "  - *Parallel Processing*: X·ª≠ l√Ω song song ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω.\n",
    "  - *A/B Testing*: Th·ª≠ nghi·ªám c√°c c·∫•u h√¨nh kh√°c nhau ƒë·ªÉ ch·ªçn ra ph∆∞∆°ng √°n t·ªëi ∆∞u.\n",
    "\n",
    "---\n",
    "\n",
    "Vi·ªác √°p d·ª•ng c√°c c√¥ng c·ª• v√† ƒë·ªëi t√°c ph√π h·ª£p trong t·ª´ng giai ƒëo·∫°n c·ªßa ki·∫øn tr√∫c RAG s·∫Ω gi√∫p b·∫°n x√¢y d·ª±ng m·ªôt h·ªá th·ªëng hi·ªáu qu·∫£, ch√≠nh x√°c v√† d·ªÖ d√†ng m·ªü r·ªông. N·∫øu b·∫°n c·∫ßn h·ªó tr·ª£ th√™m v·ªÅ tri·ªÉn khai c·ª• th·ªÉ ho·∫∑c v√≠ d·ª• m√£ ngu·ªìn, h√£y cho t√¥i bi·∫øt! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D∆∞·ªõi ƒë√¢y l√† b·∫£ng t·ªïng h·ª£p c√°c best practices (th·ª±c ti·ªÖn t·ªët nh·∫•t) cho t·ª´ng giai ƒëo·∫°n trong ki·∫øn tr√∫c RAG (Retrieval-Augmented Generation) s·ª≠ d·ª•ng LangChain, ƒë∆∞·ª£c chia th√†nh ba nh√≥m ch√≠nh:\n",
    "\n",
    "---\n",
    "\n",
    "### üß© 1. Offline Preparation (Chu·∫©n b·ªã D·ªØ li·ªáu)\n",
    "\n",
    "| Th√†nh ph·∫ßn | Best Practices | C√¥ng c·ª•/H·ªá sinh th√°i h·ªó tr·ª£ |\n",
    "|------------|----------------|-----------------------------|\n",
    "| **Data Ingestion & Chunking** | - L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu tr∆∞·ªõc khi x·ª≠ l√Ω<br>- S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c chunking ph√π h·ª£p (theo ƒëo·∫°n vƒÉn, c√¢u, ng·ªØ nghƒ©a)<br>- ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc chunk ƒë·ªÉ c√¢n b·∫±ng gi·ªØa ƒë·ªô d√†i ng·ªØ c·∫£nh v√† gi·ªõi h·∫°n token c·ªßa m√¥ h√¨nh | - LangChain Document Loaders<br>- Text Splitters (e.g., RecursiveCharacterTextSplitter, SemanticChunker)<br>- Unstructured.io, spaCy, NLTK |\n",
    "| **Embedding & Vector Store** | - Ch·ªçn m√¥ h√¨nh embedding ph√π h·ª£p v·ªõi ng√¥n ng·ªØ v√† lƒ©nh v·ª±c<br>- S·ª≠ d·ª•ng vector store t·ªëi ∆∞u cho hi·ªáu su·∫•t v√† kh·∫£ nƒÉng m·ªü r·ªông<br>- K·∫øt h·ª£p t√¨m ki·∫øm vector v√† t·ª´ kh√≥a ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c | - OpenAI Embeddings, SentenceTransformers<br>- Vector Stores: FAISS, Chroma, Pinecone, Weaviate<br>- Hybrid Search (k·∫øt h·ª£p BM25 v√† Vector Search) |\n",
    "\n",
    "---\n",
    "\n",
    "### üîç 2. Online Query Processing (X·ª≠ l√Ω Truy v·∫•n Tr·ª±c tuy·∫øn)\n",
    "\n",
    "| Th√†nh ph·∫ßn | Best Practices | C√¥ng c·ª•/H·ªá sinh th√°i h·ªó tr·ª£ |\n",
    "|------------|----------------|-----------------------------|\n",
    "| **Retrieval Strategies** | - T√πy ch·ªânh retriever v·ªõi c√°c tham s·ªë ph√π h·ª£p<br>- S·ª≠ d·ª•ng hybrid retriever ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa nhi·ªÅu ph∆∞∆°ng ph√°p<br>- √Åp d·ª•ng k·ªπ thu·∫≠t reranking ƒë·ªÉ s·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ truy xu·∫•t | - LangChain Retrievers (e.g., BM25Retriever, EnsembleRetriever)<br>- Contextual Compression Retriever<br>- Rerankers (e.g., LLMChainExtractor) |\n",
    "| **Prompt Engineering & Generation** | - Thi·∫øt k·∫ø prompt r√µ r√†ng v√† c·ª• th·ªÉ<br>- S·ª≠ d·ª•ng template prompt ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n<br>- T·ªëi ∆∞u h√≥a ƒë·ªô d√†i prompt ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n token | - LangChain Prompt Templates<br>- LangChain Chains/Graphs<br>- LLMs: OpenAI (GPT-4, GPT-3.5), Anthropic, Cohere |\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ 3. Evaluation and Deployment (ƒê√°nh gi√° v√† Tri·ªÉn khai)\n",
    "\n",
    "| Th√†nh ph·∫ßn | Best Practices | C√¥ng c·ª•/H·ªá sinh th√°i h·ªó tr·ª£ |\n",
    "|------------|----------------|-----------------------------|\n",
    "| **Evaluation & Monitoring** | - ƒê√°nh gi√° hi·ªáu su·∫•t h·ªá th·ªëng v·ªõi c√°c ch·ªâ s·ªë nh∆∞ ƒë·ªô ch√≠nh x√°c, ƒë·ªô bao ph·ªß, ƒë·ªô li√™n quan<br>- Theo d√µi v√† ghi log ƒë·ªÉ ph√¢n t√≠ch v√† c·∫£i thi·ªán h·ªá th·ªëng<br>- Th·ª±c hi·ªán th·ª≠ nghi·ªám A/B ƒë·ªÉ so s√°nh hi·ªáu qu·∫£ c·ªßa c√°c c·∫•u h√¨nh kh√°c nhau | - LangSmith<br>- RAGAS<br>- LangChain Evaluators |\n",
    "| **Deployment & Scalability** | - T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t v·ªõi caching, batch processing, parallel processing<br>- ƒê·∫£m b·∫£o kh·∫£ nƒÉng m·ªü r·ªông ƒë·ªÉ x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu v√† truy v·∫•n l·ªõn<br>- ƒê·∫£m b·∫£o b·∫£o m·∫≠t v√† quy·ªÅn ri√™ng t∆∞ c·ªßa d·ªØ li·ªáu | - FastAPI, Streamlit, Next.js<br>- LangGraph<br>- Docker, Kubernetes |\n",
    "\n",
    "---\n",
    "\n",
    "Vi·ªác √°p d·ª•ng c√°c best practices tr√™n s·∫Ω gi√∫p b·∫°n x√¢y d·ª±ng m·ªôt h·ªá th·ªëng RAG hi·ªáu qu·∫£, ch√≠nh x√°c v√† d·ªÖ d√†ng m·ªü r·ªông. N·∫øu b·∫°n c·∫ßn h·ªó tr·ª£ th√™m v·ªÅ tri·ªÉn khai c·ª• th·ªÉ ho·∫∑c v√≠ d·ª• m√£ ngu·ªìn, h√£y cho t√¥i bi·∫øt! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
